{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# US Immigration Analytics\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "Implements analytical tables that can answer different queries regarding immigration to different states in the US.\n",
    "\n",
    "Project steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "Explain what you plan to do in the project in more detail. \n",
    "What data do you use? \n",
    "What is your end solution look like? What tools did you use? etc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Project goal:**\n",
    "- Build a star schema with the immigration data as a fact table and dates, airport code, us city demographic and world tempreture as dimension tables.\n",
    "- This will allow varies kind of analysis on the immigration data, for example what is the destination state for most USA immigrants that came from Egypt.\n",
    "- I will restructure the data in the workspace using Spark, then I'm going to upload the structured data into AWS S3 using boto, finally I will create the star schema on AWS Redshift.\n",
    "- I will access the tables from python and perform my queries on AWS Redshift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "I'm using 4 datasets provided by Udacity:\n",
    " - **I94 Immigration Data**\n",
    "   - **Source:** https://travel.trade.gov/research/reports/i94/historical/2016.html\n",
    "   - **Path in Udacity workspace:** ../../data/18-83510-I94-Data-2016/\n",
    "   - There's a file for each month of the year, e.g. i94_apr16_sub.sas7bdat\n",
    "   - **Full file path:** ../../data/18-83510-I94-Data-2016/i94_jun16_sub.sas7bdat\n",
    "   - These files are large\n",
    "   - Data are reported in a **monthly** publication highlighting overseas visitor arrivals by country of residence, ports of entry, mode of transportation, type of visa, and more.\n",
    "   - **Description:** https://travel.trade.gov/research/programs/i94/description.asp , https://travel.trade.gov/research/programs/i94/index.asp\n",
    "   - **Size:** 6GB\n",
    "   - **Count:** 40,790,529\n",
    "   - 29 Columns:\n",
    "| Column   | Type   | Description                                                                                                                                                                                                                    |\n",
    "|----------|--------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| cicid    | double | Id, Part of the composite primary key                                                                                                                                                                                          |\n",
    "| i94yr    | double | 4 digit year of the arrival, Part of the Composite primary key                                                                                                                                                                 |\n",
    "| i94mon   | double | Numeric month of the arrival, Part of the composite primary key                                                                                                                                                                |\n",
    "| i94cit   | double | Visitor country of citizenship (3 digit code of origin city)                                                                                                                                                                   |\n",
    "| i94res   | double | Visitor country of residence                                                                                                                                                                                                   |\n",
    "| i94port  | string | Port of entry, can be in the USA or outside USA. (City, State if USA/Country otherwise)  3 character code of destination city  --> Foreign key City Temperature (City) --> Foreign key U.S. City Demographics (City and State) |\n",
    "| arrdate  | double | Arrival Date in the USA, It is a SAS date numeric field                                                                                                                                                                        |\n",
    "| i94mode  | double | Port mode (Air, Sea, Land, Not reported + missing values). 1 digit travel code                                                                                                                                                 |\n",
    "| i94addr  | string | Landing state. --> Foreign Key to the U.S. City Demographics (State)                                                                                                                                                           |\n",
    "| depdate  | double | Departure Date from the USA. It is a SAS date numeric field                                                                                                                                                                    |\n",
    "| i94bir   | double | Age of Respondent in Years                                                                                                                                                                                                     |\n",
    "| i94visa  | double | Visa code (Business, Pleasure, Student). Reason for immigration                                                                                                                                                                |\n",
    "| count    | double | -Used for summary statistics (I think they might consider a group of visitor as 1 record and this field represent the count of the group?)                                                                                     |\n",
    "| dtadfile | string | -Character Date Field - Date added to I-94 Files                                                                                                                                                                               |\n",
    "| visapost | string | Department of State where Visa was issued (Where the visa was issued before arriving to USA)                                                                                                                                   |\n",
    "| occup    | string | -Occupation that will be performed in U.S. (Profession)                                                                                                                                                                        |\n",
    "| entdepa  | string | -Arrival Flag - admitted or paroled into the U.S.                                                                                                                                                                              |\n",
    "| entdepd  | string | -Departure Flag - Departed, lost I-94 or is deceased                                                                                                                                                                           |\n",
    "| entdepu  | string | -Update Flag - Either apprehended, overstayed, adjusted to perm residence                                                                                                                                                      |\n",
    "| matflag  | string | Match flag - Match of arrival and departure records (If value missing -> No depature date)                                                                                                                                     |\n",
    "| biryear  | double | 4 digit year of birth                                                                                                                                                                                                          |\n",
    "| dtaddto  | string | Character Date Field - Date to which admitted to U.S. (allowed to stay until)                                                                                                                                                  |\n",
    "| gender   | string | Non-immigrant sex                                                                                                                                                                                                              |\n",
    "| insnum   | string | -INS number                                                                                                                                                                                                                    |\n",
    "| airline  | string | Code of the Airline used to arrive in U.S.  --> Foreign Key to Airport Codes, IATA, and Local Code?  --> Should also use municipality column in Airport Codes                                                                  |\n",
    "| admnum   | double | Admission Number is the number on a CBP Form I–94 or CBP Form I–94W, Does this mean it's unique? The answer is no                                                                                                              |\n",
    "| fltno    | string | Flight number of Airline used to arrive in U.S.                                                                                                                                                                                |\n",
    "| visatype | string | (B1, B2 .. etc) Class of admission legally admitting the non-immigrant to temporarily stay in U.S.                                                                                                                             |\n",
    "   - There is also a data dictionary table provided for this dataset ***I94_SAS_Labels_Descriptions.SAS***, it contains some code to value mapping, and I will create a table for each map:\n",
    "     - **I94CIT & I94RES:** 3 digits maps to a country name\n",
    "     - **I94PORT:** 3 characters maps to city and state code or country\n",
    "     - **I94MODE:** 1 digit maps to port type (Air, Sea, Land, Not reported)\n",
    "     - **I94ADDR:** 2 characters maps to a USA state name\n",
    "     - **I94VISA:** 1 digit maps to a visa category (Business, Pleasure, Student)\n",
    "     \n",
    "\n",
    "  \n",
    " - **World Temperature Data**\n",
    "   - **Source:** https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data\n",
    "   - **Path:** ../../data2/\n",
    "   - **One file:** GlobalLandTemperaturesByCity.csv\n",
    "   - **Size:** 509MB\n",
    "   - **Count:** 8,599,212\n",
    "   - 7 Columns\n",
    "| Column                        | Type   | Description                                                      |\n",
    "|-------------------------------|--------|------------------------------------------------------------------|\n",
    "| dt                            | date   | Date: starts in 1750 for average land temperature (Can be empty) |\n",
    "| AverageTemperature            | double | Global average land temperature in celsius (Can be empty)        |\n",
    "| AverageTemperatureUncertainty | double | The 95% confidence interval around the average (Can be empty)    |\n",
    "| City                          | string | 3448 Unique values                                               |\n",
    "| Country                       | string |                                                                  |\n",
    "| Latitude                      | string |                                                                  |\n",
    "| Longitude                     | string |                                                                  |\n",
    "\n",
    "  \n",
    " - **U.S. City Demographic Data**\n",
    "   - **Source:** https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/\n",
    "   - **Path:** us-cities-demographics.csv\n",
    "   - **Size:** 252KB\n",
    "   - **Count:** 2,891\n",
    "   - This dataset contains information about the demographics of all US cities and census-designated places with a population greater or equal to 65,000.\n",
    "   - 12 Columns \n",
    "| Column                 | Type   | Description                                                                                                        |\n",
    "|------------------------|--------|--------------------------------------------------------------------------------------------------------------------|\n",
    "| City                   | string | City within USA                                                                                                    |\n",
    "| State                  | string | The USA State of the city                                                                                          |\n",
    "| Median Age             | double | Median of the Ages of the people living in that city                                                               |\n",
    "| Male Population        | int    | Number of males in the city (Can be null) (Same across different records with same city)                           |\n",
    "| Female Population      | int    | of females in the city (Can be null) (Same across different records with same city)                                |\n",
    "| Total Population       | int    | Sum of male and female population (Same across different records with same city)                                   |\n",
    "| Number of Veterans     | int    | USA Veterans count                                                                                                 |\n",
    "| Foreign-born           | int    | Number of foreign born americans                                                                                   |\n",
    "| Average Household Size | double | Average number of a household                                                                                      |\n",
    "| State Code             | string | Code of the sate                                                                                                   |\n",
    "| Race                   | string | Race of the count (White, Asian, American Indian and Alaska Native, Hispanic or Latino, Black or African-American) |\n",
    "| Count                  | int    | Count of people from the race (Races can be mixed also)                                                            |\n",
    "\n",
    "\n",
    "  \n",
    " - **Airport Codes Data**\n",
    "   - **Source:** https://datahub.io/core/airport-codes#data\n",
    "   - **Path:** airport-codes_csv.csv\n",
    "   - Simple table of airport codes and corresponding cities\n",
    "   - This data is updated **nightly**.\n",
    "   - **Size:** 5.8 MB\n",
    "   - **Count:** 55,075\n",
    "   - The airport codes may refer to either IATA airport code, a three-letter code which is used in passenger reservation, ticketing and baggage-handling systems, or the ICAO airport code which is a four letter code used by ATC systems and for airports that do not have an IATA airport code\n",
    "   - 12 Columns\n",
    "| Column       | Type   | Description                                                                                         |\n",
    "|--------------|--------|-----------------------------------------------------------------------------------------------------|\n",
    "| ident        | string | -Identifier might be the airport code? - nope, iata and local codes are the airport codes           |\n",
    "| type         | string | Airport type [heliport,small_airport,closed,seaplane_base,balloonport,medium_airport,large_airport] |\n",
    "| name         | string | Airport name                                                                                        |\n",
    "| elevation_ft | int    | Elavation of the airport from sea level                                                             |\n",
    "| continent    | string | Code for continent of the Airport [NaN, OC, AF, AN, EU, AS, SA]                                     |\n",
    "| iso_country  | string | Code for country of the Airport                                                                     |\n",
    "| iso_region   | string | Code for region of the Airport                                                                      |\n",
    "| municipality | string | City of the Airport                                                                                 |\n",
    "| gps_code     | string | Airport GPS code (Contains nulls)                                                                   |\n",
    "| iata_code    | string | IATA airport code (Contains nulls)                                                                  |\n",
    "| local_code   | string | ICAO airport code (Contains nulls)                                                                  |\n",
    "| coordinates  | string | Airport coordinates lat., long. comma separated                                                     |\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "- I will do a full analysis on the data sets here to understand how can I use them to create the a conceptual model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the sample immigration data into a Pandas DataFrame\n",
    "#fname = 'immigration_data_sample.csv'\n",
    "#df = pd.read_csv(fname)\n",
    "#df.info()\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read SaS file using Spark\n",
    "#df_spark =spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n",
    "#df_spark.printSchema()\n",
    "\n",
    "#write to parquet\n",
    "#df_spark.write.parquet(\"i94_immigration/i94_apr16_sub\")\n",
    "#df_spark=spark.read.parquet(\"i94_immigration/i94_apr16_sub\")\n",
    "#df_spark.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob   # For reading all files in a directory\n",
    "import ntpath # For reading file name from path\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### i94_immigration, checking schema to decide what types need to be changed\n",
    "- **cicid** type to long\n",
    "- **i94yr, i94mon, i94cit, i94res, i94mode, i94bir, i94visa** to int \n",
    "- **arrdate, depdate** to date\n",
    "- **matflag** has 2 values, (null, 'M'), need to be altered later to boolean\n",
    "- **biryear** to int\n",
    "- **dmnum** type to long\n",
    "- I'm not interested in the following columns **[count,dtadfile,visapost,occup,entdepa,entdepd,entdepu,insnum]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|  6.0|2016.0|   4.0| 692.0| 692.0|    XXX|20573.0|   null|   null|   null|  37.0|    2.0|  1.0|    null|    null| null|      T|   null|      U|   null| 1979.0|10282016|  null|  null|   null| 1.897628485E9| null|      B2|\n",
      "|  7.0|2016.0|   4.0| 254.0| 276.0|    ATL|20551.0|    1.0|     AL|   null|  25.0|    3.0|  1.0|20130811|     SEO| null|      G|   null|      Y|   null| 1991.0|     D/S|     M|  null|   null|  3.73679633E9|00296|      F1|\n",
      "| 15.0|2016.0|   4.0| 101.0| 101.0|    WAS|20545.0|    1.0|     MI|20691.0|  55.0|    2.0|  1.0|20160401|    null| null|      T|      O|   null|      M| 1961.0|09302016|     M|  null|     OS|  6.66643185E8|   93|      B2|\n",
      "| 16.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     MA|20567.0|  28.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1988.0|09302016|  null|  null|     AA|9.246846133E10|00199|      B2|\n",
      "| 17.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     MA|20567.0|   4.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 2012.0|09302016|  null|  null|     AA|9.246846313E10|00199|      B2|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking schema to decide what types need to be changed\n",
    "df_spark =spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n",
    "df_spark.printSchema()\n",
    "df_spark.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop unwanted columns\n",
    "cols_to_drop = ['count','dtadfile','visapost','occup','entdepa','entdepd','entdepu','insnum']\n",
    "df_spark = df_spark.drop(*cols_to_drop)\n",
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Used for converting matflag to boolean\n",
    "@udf(\"boolean\")\n",
    "def isMatch(x):\n",
    "    return (True if x == 'M' else False)\n",
    "\n",
    "# Used for Unifying null and 0 values to be 9 instead for i94mode \n",
    "@udf(\"integer\")\n",
    "def isModeNull(x):\n",
    "    return (9 if (x == None) or (x == 0) else x)\n",
    "\n",
    "# Used to convert SAS date into DateType\n",
    "def convert_datetime(x):\n",
    "    try:\n",
    "        start = datetime(1960, 1, 1)\n",
    "        return start + timedelta(days=int(x))\n",
    "    except:\n",
    "        return None\n",
    "udf_datetime_from_sas = udf(lambda x: convert_datetime(x), DateType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: long (nullable = true)\n",
      " |-- i94yr: integer (nullable = true)\n",
      " |-- i94mon: integer (nullable = true)\n",
      " |-- i94cit: integer (nullable = true)\n",
      " |-- i94res: integer (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: date (nullable = true)\n",
      " |-- i94mode: integer (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: date (nullable = true)\n",
      " |-- i94bir: integer (nullable = true)\n",
      " |-- i94visa: integer (nullable = true)\n",
      " |-- biryear: integer (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: long (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      " |-- matflag: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Alter some columns types:\n",
    "# cicid type to long\n",
    "df_spark = df_spark.withColumn(\"cicid\", df_spark[\"cicid\"].cast(LongType()))\n",
    "# i94yr, i94mon, i94cit, i94res, arrdate, i94mode, depdate, i94bir, i94visa to int \n",
    "df_spark = df_spark.withColumn(\"i94yr\", df_spark[\"i94yr\"].cast(IntegerType()))\n",
    "df_spark = df_spark.withColumn(\"i94mon\", df_spark[\"i94mon\"].cast(IntegerType()))\n",
    "df_spark = df_spark.withColumn(\"i94cit\", df_spark[\"i94cit\"].cast(IntegerType()))\n",
    "df_spark = df_spark.withColumn(\"i94res\", df_spark[\"i94res\"].cast(IntegerType()))\n",
    "\n",
    "df_spark = df_spark.withColumn(\"arrdate\", udf_datetime_from_sas(\"arrdate\"))\n",
    "\n",
    "df_spark = df_spark.withColumn(\"i94mode\", df_spark[\"i94mode\"].cast(IntegerType()))\n",
    "# Use 9 instead of null for missing values, use 9 instead of 0 for Not reported values\n",
    "df_spark = df_spark.withColumn(\"i94mode\", isModeNull(df_spark[\"i94mode\"]))\n",
    "\n",
    "df_spark = df_spark.withColumn(\"depdate\", udf_datetime_from_sas(\"depdate\"))\n",
    "\n",
    "df_spark = df_spark.withColumn(\"i94bir\", df_spark[\"i94bir\"].cast(IntegerType()))\n",
    "df_spark = df_spark.withColumn(\"i94bir\", df_spark[\"i94bir\"].cast(IntegerType()))\n",
    "df_spark = df_spark.withColumn(\"i94visa\", df_spark[\"i94visa\"].cast(IntegerType()))\n",
    "# matflag has 2 values, (null, 'M'), to boolean\n",
    "df_spark = df_spark.withColumn(\"match_flag\",isMatch(df_spark[\"matflag\"]))\n",
    "df_spark = df_spark.drop(\"matflag\")\n",
    "df_spark = df_spark.withColumnRenamed(\"match_flag\",\"matflag\")\n",
    "# biryear to int\n",
    "df_spark = df_spark.withColumn(\"biryear\", df_spark[\"biryear\"].cast(IntegerType()))                                                \n",
    "# admnum type to long\n",
    "df_spark = df_spark.withColumn(\"admnum\", df_spark[\"admnum\"].cast(LongType()))\n",
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Function to restructure the dataframe\n",
    "def restructure_i94(df):\n",
    "    # Drop not needed columns - Noticed some new columns in one of the months, [validres,delete_days,delete_mexl,delete_dup,delete_visa,delete_recdup]\n",
    "    cols_to_drop = ['count','dtadfile','visapost','occup','entdepa','entdepd','entdepu','insnum','validres','delete_days','delete_mexl','delete_dup','delete_visa','delete_recdup']\n",
    "    df = df.drop(*cols_to_drop)\n",
    "    # Alter some columns types:\n",
    "    # cicid type to long\n",
    "    df = df.withColumn(\"cicid\", df[\"cicid\"].cast(LongType()))\n",
    "    # i94yr, i94mon, i94cit, i94res, arrdate, i94mode, depdate, i94bir, i94visa to int \n",
    "    df = df.withColumn(\"i94yr\", df[\"i94yr\"].cast(IntegerType()))\n",
    "    df = df.withColumn(\"i94mon\", df[\"i94mon\"].cast(IntegerType()))\n",
    "    df = df.withColumn(\"i94cit\", df[\"i94cit\"].cast(IntegerType()))\n",
    "    df = df.withColumn(\"i94res\", df[\"i94res\"].cast(IntegerType()))\n",
    "    \n",
    "    df = df.withColumn(\"arrdate\", udf_datetime_from_sas(\"arrdate\"))\n",
    "    \n",
    "    df = df.withColumn(\"i94mode\", df[\"i94mode\"].cast(IntegerType()))\n",
    "    # Use 8 instead of null for missing values\n",
    "    df = df.withColumn(\"i94mode\", isModeNull(df[\"i94mode\"]))\n",
    "    \n",
    "    df = df.withColumn(\"depdate\", udf_datetime_from_sas(\"depdate\"))\n",
    "    \n",
    "    df = df.withColumn(\"i94bir\", df[\"i94bir\"].cast(IntegerType()))\n",
    "    df = df.withColumn(\"i94bir\", df[\"i94bir\"].cast(IntegerType()))\n",
    "    df = df.withColumn(\"i94visa\", df[\"i94visa\"].cast(IntegerType()))\n",
    "    # matflag has 2 values, (null, 'M'), to boolean\n",
    "    df = df.withColumn(\"match_flag\",isMatch(df[\"matflag\"]))\n",
    "    df = df.drop(\"matflag\")\n",
    "    df = df.withColumnRenamed(\"match_flag\",\"matflag\")\n",
    "    # biryear to int\n",
    "    df = df.withColumn(\"biryear\", df[\"biryear\"].cast(IntegerType()))                                                \n",
    "    # admnum type to long\n",
    "    df = df.withColumn(\"admnum\", df[\"admnum\"].cast(LongType()))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Convert SAS files to Parquet files and store them localy\n",
    "i94SASFiles = glob.glob(\"../../data/18-83510-I94-Data-2016/*\") # Gets all SAS files\n",
    "for i94SASFile in i94SASFiles:\n",
    "    fileName = ntpath.basename(i94SASFile).split(\".\")[0]\n",
    "    df_spark = spark.read.format('com.github.saurfang.sas.spark').load(i94SASFile)\n",
    "    df_spark = restructure_i94(df_spark)\n",
    "    df_spark.coalesce(1).write.parquet(\"i94_immigration/\"+fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total count = 40790529\n",
      "+-----+-----+------+------+------+-------+----------+-------+-------+----------+------+-------+-------+--------+------+-------+-----------+-----+--------+-------+\n",
      "|cicid|i94yr|i94mon|i94cit|i94res|i94port|   arrdate|i94mode|i94addr|   depdate|i94bir|i94visa|biryear| dtaddto|gender|airline|     admnum|fltno|visatype|matflag|\n",
      "+-----+-----+------+------+------+-------+----------+-------+-------+----------+------+-------+-------+--------+------+-------+-----------+-----+--------+-------+\n",
      "|  687| 2016|     9|   213|   213|    HOU|2016-09-01|      1|     TX|2016-09-28|    27|      1|   1989|11022016|     M|     QK|95279388530| 8111|      B1|   true|\n",
      "|  799| 2016|     9|   369|   369|    WAS|2016-09-01|      1|     VA|2016-09-28|    71|      2|   1945|11032016|     M|     KL|95310934730|  651|      B2|   true|\n",
      "| 1001| 2016|     9|   582|   582|    LVG|2016-09-01|      1|     NV|2016-09-05|    25|      2|   1991|10312016|     M|     4O|95076479830|  970|      B2|   true|\n",
      "| 1044| 2016|     9|   691|   582|    MIA|2016-09-01|      1|     FL|2016-09-05|    50|      1|   1966|11032016|     M|     AM|95329654930|  428|      B1|   true|\n",
      "| 1183| 2016|     9|   266|   266|    CHI|2016-09-01|      1|     WI|2016-09-17|    34|      3|   1982|     D/S|     F|     UA|94655935030| 5959|      F1|   true|\n",
      "+-----+-----+------+------+------+-------+----------+-------+-------+----------+------+-------+-------+--------+------+-------+-----------+-----+--------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- cicid: long (nullable = true)\n",
      " |-- i94yr: integer (nullable = true)\n",
      " |-- i94mon: integer (nullable = true)\n",
      " |-- i94cit: integer (nullable = true)\n",
      " |-- i94res: integer (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: date (nullable = true)\n",
      " |-- i94mode: integer (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: date (nullable = true)\n",
      " |-- i94bir: integer (nullable = true)\n",
      " |-- i94visa: integer (nullable = true)\n",
      " |-- biryear: integer (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: long (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      " |-- matflag: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read all i94DF parquet dataset into a Dataframe\n",
    "i94DF = None # i94 Dataframe\n",
    "i94ParquetFolders = glob.glob(\"i94_immigration/*\") \n",
    "\n",
    "i94_schema = StructType([\n",
    "    StructField('cicid', LongType(), True),\n",
    "    StructField('i94yr', IntegerType(), True),\n",
    "    StructField('i94mon', IntegerType(), True),\n",
    "    StructField('i94cit', IntegerType(), True),\n",
    "    StructField('i94res', IntegerType(), True),\n",
    "    StructField('i94port', StringType(), True),\n",
    "    StructField('arrdate', DateType(), True),\n",
    "    StructField('i94mode', IntegerType(), True),\n",
    "    StructField('i94addr', StringType(), True),\n",
    "    StructField('depdate', DateType(), True),\n",
    "    StructField('i94bir', IntegerType(), True),\n",
    "    StructField('i94visa', IntegerType(), True),\n",
    "    StructField('biryear', IntegerType(), True),\n",
    "    StructField('dtaddto', StringType(), True),\n",
    "    StructField('gender', StringType(), True),\n",
    "    StructField('airline', StringType(), True),\n",
    "    StructField('admnum', LongType(), True),\n",
    "    StructField('fltno', StringType(), True),\n",
    "    StructField('visatype', StringType(), True),\n",
    "    StructField('matflag', BooleanType(), True)\n",
    "])\n",
    "for i94ParquetFolder in i94ParquetFolders:\n",
    "    df_spark=spark.read.schema(i94_schema).parquet(i94ParquetFolder)\n",
    "    if i94DF is None:\n",
    "        i94DF = df_spark\n",
    "    else:\n",
    "        i94DF = i94DF.union(df_spark)\n",
    "# Total count of all records in i94 \n",
    "totalCount = i94DF.count()\n",
    "print(\"total count = \"+str(totalCount))\n",
    "# Show some data\n",
    "i94DF.show(5)\n",
    "i94DF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is cicid unique? False\n"
     ]
    }
   ],
   "source": [
    "# Is cicid unique?\n",
    "is_cicid_unique = (i94DF.select(\"cicid\").distinct().count() == i94DF.count())\n",
    "print(\"Is cicid unique? \"+str(is_cicid_unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|  cicid|count|\n",
      "+-------+-----+\n",
      "|5342075|    9|\n",
      "|5342416|    7|\n",
      "|5342504|    7|\n",
      "+-------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# What are the duplicated cicid ?\n",
    "duplicated_cicid_df = i94DF.groupBy('cicid').agg(count('cicid').alias('count')).where(column('count')>1)\n",
    "duplicated_cicid_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-------+--------+------+-------+-----------+-----+--------+-------+\n",
      "|  cicid|i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|biryear| dtaddto|gender|airline|     admnum|fltno|visatype|matflag|\n",
      "+-------+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-------+--------+------+-------+-----------+-----+--------+-------+\n",
      "|5342416| 2016|     9|   504|   504|    MIA|  20721|      1|     FL|   null|    31|      2|   1985|03232017|     M|     AA|12054498685|  960|      B2|  false|\n",
      "|5342416| 2016|     4|   576|   576|    MIA|  20572|      1|   null|  20610|    76|      2|   1940|10272016|     F|     AA|94793677030|01520|      B2|   true|\n",
      "|5342416| 2016|    12|   273|   273|    HOU|  20812|      1|     TX|  20829|    20|      2|   1996|06232017|     F|     SQ|18638744885|   52|      B2|   true|\n",
      "|5342416| 2016|     5|   687|   687|    MIA|  20600|      1|     FL|  20610|    53|      2|   1963|11252016|     M|     AR|97380627830|01302|      B2|   true|\n",
      "|5342416| 2016|     3|   135|   135|    CLT|  20539|      1|     FL|  20553|    48|      2|   1968|06232016|     M|     AA|55108234633|00731|      WT|   true|\n",
      "|5342416| 2016|     7|   689|   689|    BOS|  20658|      1|     MA|  20662|    50|      2|   1966|01232017|     F|     AC|17399052040|00358|      B2|   true|\n",
      "|5342416| 2016|     8|   135|   135|    NEW|  20689|      1|     NY|  20694|    18|      2|   1998|11202016|     F|     UA| 6875184385|   75|      WT|   true|\n",
      "+-------+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-------+--------+------+-------+-----------+-----+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Investigating one of the duplicates entries\n",
    "one_duplicate = i94DF.where(i94DF.cicid == 5342416)\n",
    "one_duplicate.show()\n",
    "# It looks like we have a composite primary key [cicid, i94yr, i94mon]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+-----+\n",
      "|cicid|i94yr|i94mon|count|\n",
      "+-----+-----+------+-----+\n",
      "+-----+-----+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Is [cicid, i94yr, i94mon] unique?\n",
    "composite_pk_df = i94DF.groupBy('cicid','i94yr','i94mon').agg(count('cicid').alias('count')).where(column('count')>1)\n",
    "composite_pk_df.show()\n",
    "# the answer is yes, so we can say that for i94 dataset, [cicid, i94yr, i94mon] is a composite primary key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|i94mode|\n",
      "+-------+\n",
      "|      1|\n",
      "|      3|\n",
      "|      9|\n",
      "|      2|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking if i94mode has 4 values only\n",
    "i94DF.select('i94mode').distinct().show()\n",
    "# It has 4 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|matflag|\n",
      "+-------+\n",
      "|   true|\n",
      "|  false|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking all values for matflag\n",
    "i94DF.select('matflag').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|gender|\n",
      "+------+\n",
      "|     F|\n",
      "|  null|\n",
      "|     M|\n",
      "|     U|\n",
      "|     X|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking all values for gender\n",
    "i94DF.select('gender').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is admnum unique? False\n"
     ]
    }
   ],
   "source": [
    "# is admnum unique? \n",
    "is_admnum_unique = (i94DF.select(\"admnum\").distinct().count() == i94DF.count())\n",
    "print(\"Is admnum unique? \"+str(is_admnum_unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(max(admnum)=99999998130)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is the maximum admnum\n",
    "i94DF.agg({\"admnum\": \"max\"}).collect()[0]\n",
    "# This need to be casted to Long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Creating mapping files for these columns:\n",
    "- **I94CIT & I94RES:** 3 digits maps to a country name\n",
    "- **I94PORT        :** 3 characters maps to city and state code or country\n",
    "- **I94MODE        :** 1 digit maps to port type (Air, Sea, Land, Not reported)\n",
    "- **I94ADDR        :** 2 characters maps to a USA state name\n",
    "- **I94VISA        :** 1 digit maps to a visa category (Business, Pleasure, Student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- code: integer (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      "\n",
      "+----+-----------+\n",
      "|code|    country|\n",
      "+----+-----------+\n",
      "| 582|     MEXICO|\n",
      "| 236|AFGHANISTAN|\n",
      "| 101|    ALBANIA|\n",
      "+----+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  - I94CIT & I94RES: 3 digits maps to a country name\n",
    "code_country_schema = StructType([\n",
    "    StructField('code', IntegerType(), True),\n",
    "    StructField('country', StringType(), True)\n",
    "])\n",
    "code_country_df = spark.read.option(\"quote\",\"\\\"\").csv(\"lookups/code_country.csv\",schema = code_country_schema,header = True)\n",
    "code_country_df.printSchema()\n",
    "code_country_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- code: string (nullable = true)\n",
      " |-- port: string (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      "\n",
      "+----+--------------------+----------+\n",
      "|code|                port|state_code|\n",
      "+----+--------------------+----------+\n",
      "| ALC|               ALCAN|        AK|\n",
      "| ANC|           ANCHORAGE|        AK|\n",
      "| BAR|BAKER AAF - BAKER...|        AK|\n",
      "+----+--------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  - I94PORT        : 3 characters maps to city and state code or country\n",
    "ports_schema = StructType([\n",
    "    StructField('code', StringType(), True),\n",
    "    StructField('port', StringType(), True),\n",
    "    StructField('state_code', StringType(), True)\n",
    "])\n",
    "ports_df = spark.read.option(\"quote\",\"\\\"\").csv(\"lookups/ports.csv\",schema = ports_schema,header = True)\n",
    "ports_df.printSchema()\n",
    "ports_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- code: integer (nullable = true)\n",
      " |-- mode: string (nullable = true)\n",
      "\n",
      "+----+------------+\n",
      "|code|        mode|\n",
      "+----+------------+\n",
      "|   1|         Air|\n",
      "|   2|         Sea|\n",
      "|   3|        Land|\n",
      "|   9|Not reported|\n",
      "+----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  - I94MODE        : 1 digit maps to port type (Air, Sea, Land, Not reported)\n",
    "port_mode_schema = StructType([\n",
    "    StructField('code', IntegerType(), True),\n",
    "    StructField('mode', StringType(), True),\n",
    "])\n",
    "port_mode_df = spark.read.option(\"quote\",\"\\\"\").option(\"nullValue\",\"null\").csv(\"lookups/port_mode.csv\",schema = port_mode_schema,header = True)\n",
    "port_mode_df.printSchema()\n",
    "port_mode_df.show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- state_code: string (nullable = true)\n",
      " |-- state_name: string (nullable = true)\n",
      "\n",
      "+----------+----------+\n",
      "|state_code|state_name|\n",
      "+----------+----------+\n",
      "|        AL|   ALABAMA|\n",
      "|        AK|    ALASKA|\n",
      "|        AZ|   ARIZONA|\n",
      "+----------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  - I94ADDR        : 2 characters maps to a USA state name\n",
    "states_schema = StructType([\n",
    "    StructField('state_code', StringType(), True),\n",
    "    StructField('state_name', StringType(), True),\n",
    "])\n",
    "states_df = spark.read.option(\"quote\",\"\\\"\").csv(\"lookups/states.csv\",schema = states_schema,header = True)\n",
    "states_df.printSchema()\n",
    "states_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- code: integer (nullable = true)\n",
      " |-- visa_category: string (nullable = true)\n",
      "\n",
      "+----+-------------+\n",
      "|code|visa_category|\n",
      "+----+-------------+\n",
      "|   1|     Business|\n",
      "|   2|     Pleasure|\n",
      "|   3|      Student|\n",
      "+----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  - I94VISA        : 1 digit maps to a visa category (Business, Pleasure, Student)\n",
    "visa_category_schema = StructType([\n",
    "    StructField('code', IntegerType(), True),\n",
    "    StructField('visa_category', StringType(), True),\n",
    "])\n",
    "visa_category_df = spark.read.option(\"quote\",\"\\\"\").csv(\"lookups/visa_category.csv\",schema = visa_category_schema,header = True)\n",
    "visa_category_df.printSchema()\n",
    "visa_category_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----+-------+\n",
      "|cicid|i94res|code|country|\n",
      "+-----+------+----+-------+\n",
      "+-----+------+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Checking if i94 data matches the lookup columns\n",
    "# Checking for \"I94CIT & I94RES\"\n",
    "i94_origin_country = i94DF.join(code_country_df,(i94DF.i94cit == code_country_df.code) | (i94DF.i94res == code_country_df.code),how='left')\\\n",
    "                    .select(i94DF.cicid,i94DF.i94res,code_country_df.code,code_country_df.country)\n",
    "i94_origin_country.where(i94_origin_country.code.isNull()).show()\n",
    "# Perfect, all codes match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----+----------+\n",
      "|i94port|code|port|state_code|\n",
      "+-------+----+----+----------+\n",
      "+-------+----+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking for \"I94PORT\"\n",
    "i94_ports = i94DF.join(ports_df,i94DF.i94port == ports_df.code,how='left')\\\n",
    "                    .select(i94DF.i94port,ports_df.code,ports_df.port,ports_df.state_code)\n",
    "i94_ports.where(i94_ports.code.isNull()).show()\n",
    "# Code: OCA was missing, after searching I found out that it corresponds to \"Ocean Reef Club Airport\" in Florida, so I added it as a new entry in ports.csv\n",
    "# Now all codes match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----+\n",
      "|i94mode|code|mode|\n",
      "+-------+----+----+\n",
      "+-------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking for \"I94MODE\"\n",
    "i94_port_mode = i94DF.join(port_mode_df,i94DF.i94mode == port_mode_df.code,how='left')\\\n",
    "                    .select(i94DF.i94mode,port_mode_df.code,port_mode_df.mode)\n",
    "i94_port_mode.where(i94_port_mode.code.isNull()).show()\n",
    "# All match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|i94addr|\n",
      "+-------+\n",
      "|     .N|\n",
      "|     07|\n",
      "|     DZ|\n",
      "|      K|\n",
      "|     LT|\n",
      "+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking for \"I94ADDR\"\n",
    "i94_states = i94DF.join(states_df,i94DF.i94addr == states_df.state_code,how='left')\\\n",
    "                    .select(i94DF.i94addr,states_df.state_code,states_df.state_name)\n",
    "i94_states.select('i94addr').distinct().where(i94_states.state_code.isNull()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "730"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many non matching i94addr?\n",
    "i94_states.select('i94addr').distinct().where(i94_states.state_code.isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-------------+\n",
      "|i94visa|code|visa_category|\n",
      "+-------+----+-------------+\n",
      "+-------+----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking for \"I94VISA\"\n",
    "i94_visa_category = i94DF.join(visa_category_df,i94DF.i94visa == visa_category_df.code,how='left')\\\n",
    "                    .select(i94DF.i94visa,visa_category_df.code,visa_category_df.visa_category)\n",
    "i94_visa_category.where(i94_visa_category.code.isNull()).show()\n",
    "# All match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+------------+------------+\n",
      "|min(arrdate)|max(arrdate)|min(depdate)|max(depdate)|\n",
      "+------------+------------+------------+------------+\n",
      "|  2016-01-01|  2016-12-31|  1920-08-10|  2092-05-09|\n",
      "+------------+------------+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# What are minimum and maximum arrival date?\n",
    "i94DF.agg(min(\"arrdate\"),max(\"arrdate\"),min(\"depdate\"),max(\"depdate\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Analysing World Tempreture Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dt: date (nullable = true)\n",
      " |-- AverageTemperature: double (nullable = true)\n",
      " |-- AverageTemperatureUncertainty: double (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      "\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|        dt|AverageTemperature|AverageTemperatureUncertainty| City|Country|Latitude|Longitude|\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|1743-11-01|             6.068|           1.7369999999999999|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1743-12-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-01-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-02-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-03-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "worldTemp_schema = StructType([\n",
    "    StructField('dt', DateType(), True),\n",
    "    StructField('AverageTemperature', DoubleType(), True),\n",
    "    StructField('AverageTemperatureUncertainty', DoubleType(), True),\n",
    "    StructField('City', StringType(), True),\n",
    "    StructField('Country', StringType(), True),\n",
    "    StructField('Latitude', StringType(), True),\n",
    "    StructField('Longitude', StringType(), True)\n",
    "])\n",
    "worldTempDF = spark.read.csv(\"../../data2/\",schema = worldTemp_schema,header = True)\n",
    "worldTempDF.printSchema()\n",
    "worldTempDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dt: date (nullable = true)\n",
      " |-- AverageTemperature: double (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n",
      "+----------+------------------+-----+-------+\n",
      "|        dt|AverageTemperature| City|Country|\n",
      "+----------+------------------+-----+-------+\n",
      "|1743-11-01|             6.068|Århus|Denmark|\n",
      "|1743-12-01|              null|Århus|Denmark|\n",
      "|1744-01-01|              null|Århus|Denmark|\n",
      "|1744-02-01|              null|Århus|Denmark|\n",
      "|1744-03-01|              null|Århus|Denmark|\n",
      "+----------+------------------+-----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# I don't need columns [AverageTemperatureUncertainty,Latitude,Longitude]\n",
    "worldTempDF = worldTempDF.drop(\"AverageTemperatureUncertainty\",\"Latitude\",\"Longitude\")\n",
    "worldTempDF.printSchema()\n",
    "worldTempDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8599212"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worldTempDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|           City|\n",
      "+---------------+\n",
      "|     Charleston|\n",
      "|         Corona|\n",
      "|    Springfield|\n",
      "|          Tempe|\n",
      "|North Las Vegas|\n",
      "+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "worldTempDF.select(\"City\").distinct().where(worldTempDF.Country == \"United States\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Get Min and Max dt\n",
    "- This shows that the tempreture dataset doesn't overlap with the i94 dataset, that's kinda disappointing.\n",
    "- At least we can get the average of tempretures per city, country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|   min(dt)|   max(dt)|\n",
      "+----------+----------+\n",
      "|1743-11-01|2013-09-01|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "worldTempDF.agg(min(\"dt\"),max(\"dt\")).show()\n",
    "# This shows that the tempreture dataset doesn't overlap with the i94 dataset, that's kinda disappointing.\n",
    "# At least we can get the average of tempretures per city, country"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Create a new dataset out of world tempreture dataset with average tempreture per city and per country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+------------------+\n",
      "|        City|           Country|           AvgTemp|\n",
      "+------------+------------------+------------------+\n",
      "|   Allentown|     United States| 9.523295607566514|\n",
      "|      Atyrau|        Kazakhstan|  8.06841197046249|\n",
      "|     Bintulu|          Malaysia|26.156729436109686|\n",
      "| Butterworth|          Malaysia|27.212006301502615|\n",
      "|      Cainta|       Philippines|26.448334487877297|\n",
      "|      Ciamis|         Indonesia|24.768451303885037|\n",
      "|      Dodoma|          Tanzania| 22.18983868935102|\n",
      "|      Fuling|             China|16.844962073931832|\n",
      "|      Fuyang|             China| 15.14607384169885|\n",
      "|         Ife|           Nigeria|26.373105171411947|\n",
      "|  Jhunjhunun|             India|25.178274018379312|\n",
      "|      Maxixe|        Mozambique|24.087632069970855|\n",
      "|      Owerri|           Nigeria| 26.61457809798272|\n",
      "|Puerto Plata|Dominican Republic|26.035798296727982|\n",
      "| Santo André|            Brazil|19.699367843511453|\n",
      "|      Soweto|      South Africa|15.069573945541919|\n",
      "|    Sukabumi|         Indonesia| 25.79808834486425|\n",
      "|    Toulouse|            France| 10.48450284270373|\n",
      "|  Tulancingo|            Mexico|15.717422377622395|\n",
      "|      Anyama|     Côte D'Ivoire|26.178451612903224|\n",
      "+------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "worldAverageTempDF = worldTempDF.where(worldTempDF.AverageTemperature.isNotNull()).groupBy(\"City\",\"Country\").agg(avg(\"AverageTemperature\").alias(\"AvgTemp\"))\n",
    "worldAverageTempDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- AvgTemp: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "worldAverageTempDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3490"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worldAverageTempDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+------------------+------------+\n",
      "|        City|           Country|           AvgTemp|Country_Code|\n",
      "+------------+------------------+------------------+------------+\n",
      "|   Allentown|     United States| 9.523295607566514|        null|\n",
      "|      Atyrau|        Kazakhstan|  8.06841197046249|         155|\n",
      "|     Bintulu|          Malaysia|26.156729436109686|         273|\n",
      "| Butterworth|          Malaysia|27.212006301502615|         273|\n",
      "|      Cainta|       Philippines|26.448334487877297|         260|\n",
      "|      Ciamis|         Indonesia|24.768451303885037|         204|\n",
      "|      Dodoma|          Tanzania| 22.18983868935102|         353|\n",
      "|      Fuling|             China|16.844962073931832|         245|\n",
      "|      Fuyang|             China| 15.14607384169885|         245|\n",
      "|         Ife|           Nigeria|26.373105171411947|         343|\n",
      "|  Jhunjhunun|             India|25.178274018379312|         213|\n",
      "|      Maxixe|        Mozambique|24.087632069970855|         329|\n",
      "|      Owerri|           Nigeria| 26.61457809798272|         343|\n",
      "|Puerto Plata|Dominican Republic|26.035798296727982|         585|\n",
      "| Santo André|            Brazil|19.699367843511453|         689|\n",
      "|      Soweto|      South Africa|15.069573945541919|         373|\n",
      "|    Sukabumi|         Indonesia| 25.79808834486425|         204|\n",
      "|    Toulouse|            France| 10.48450284270373|         111|\n",
      "|  Tulancingo|            Mexico|15.717422377622395|         582|\n",
      "|      Anyama|     Côte D'Ivoire|26.178451612903224|        null|\n",
      "+------------+------------------+------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join temp with code_country\n",
    "worldAverageTempDF = worldAverageTempDF.join(code_country_df,lower(worldAverageTempDF.Country) == lower(code_country_df.country), 'left')\\\n",
    "                            .select(worldAverageTempDF.City,\\\n",
    "                                    worldAverageTempDF.Country,\\\n",
    "                                    worldAverageTempDF.AvgTemp,\\\n",
    "                                    code_country_df.code.alias(\"Country_Code\"))\n",
    "worldAverageTempDF.show()\n",
    "# Code will be null for United states, since code_country.csv comes form the dataset of the immigration to the United states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|country      |\n",
      "+-------------+\n",
      "|Cambodia     |\n",
      "|Congo        |\n",
      "|Côte D'Ivoire|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "worldAverageTempDF.select(\"country\").distinct().filter( (column(\"Country\") != \"United States\") & column(\"Country_Code\").isNull()).show(n=10,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3490"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worldAverageTempDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- AvgTemp: double (nullable = true)\n",
      " |-- Country_Code: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "worldAverageTempDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write dataframe to parquet file\n",
    "worldAverageTempDF.coalesce(1).write.parquet(\"world_temp/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Analysing U.S. Cities Demographics Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- median_age: double (nullable = true)\n",
      " |-- male_population: integer (nullable = true)\n",
      " |-- female_population: integer (nullable = true)\n",
      " |-- total_population: integer (nullable = true)\n",
      " |-- number_of_veterans: integer (nullable = true)\n",
      " |-- foreign_born: integer (nullable = true)\n",
      " |-- average_household_size: double (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n",
      "+-------+-----+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|   city|state|median_age|male_population|female_population|total_population|number_of_veterans|foreign_born|average_household_size|state_code|                race|count|\n",
      "+-------+-----+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|Abilene|Texas|      31.3|          65212|            60664|          125876|              9367|        8129|                  2.64|        TX|               Asian| 2929|\n",
      "|Abilene|Texas|      31.3|          65212|            60664|          125876|              9367|        8129|                  2.64|        TX|American Indian a...| 1813|\n",
      "|Abilene|Texas|      31.3|          65212|            60664|          125876|              9367|        8129|                  2.64|        TX|Black or African-...|14449|\n",
      "|Abilene|Texas|      31.3|          65212|            60664|          125876|              9367|        8129|                  2.64|        TX|               White|95487|\n",
      "|Abilene|Texas|      31.3|          65212|            60664|          125876|              9367|        8129|                  2.64|        TX|  Hispanic or Latino|33222|\n",
      "|  Akron| Ohio|      38.1|          96886|           100667|          197553|             12878|       10024|                  2.24|        OH|American Indian a...| 1845|\n",
      "+-------+-----+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "usCitiesDemographics_schema = StructType([\n",
    "    StructField('city', StringType(), True),\n",
    "    StructField('state', StringType(), True),\n",
    "    StructField('median_age', DoubleType(), True),\n",
    "    StructField('male_population', IntegerType(), True),\n",
    "    StructField('female_population', IntegerType(), True),\n",
    "    StructField('total_population', IntegerType(), True),\n",
    "    StructField('number_of_veterans', IntegerType(), True),\n",
    "    StructField('foreign_born', IntegerType(), True),\n",
    "    StructField('average_household_size', DoubleType(), True),\n",
    "    StructField('state_code', StringType(), True),\n",
    "    StructField('race', StringType(), True),\n",
    "    StructField('count', IntegerType(), True)\n",
    "])\n",
    "\n",
    "usCitiesDemographicsDF = spark.read.csv(\"us-cities-demographics.csv\",schema = usCitiesDemographics_schema,header = True,sep = \";\")\n",
    "usCitiesDemographicsDF.printSchema()\n",
    "usCitiesDemographicsDF.orderBy(usCitiesDemographicsDF.city, usCitiesDemographicsDF.state).show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2891"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usCitiesDemographicsDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Create a new dataset out of U.S. Cities Demographics dataset with only the columns and rows in interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+---------------+-----------------+----------------+------------+----------+\n",
      "|   city|     state|median_age|male_population|female_population|total_population|foreign_born|state_code|\n",
      "+-------+----------+----------+---------------+-----------------+----------------+------------+----------+\n",
      "|Abilene|     Texas|      31.3|          65212|            60664|          125876|        8129|        TX|\n",
      "|  Akron|      Ohio|      38.1|          96886|           100667|          197553|       10024|        OH|\n",
      "|Alafaya|   Florida|      33.5|          39504|            45760|           85264|       15842|        FL|\n",
      "|Alameda|California|      41.4|          37747|            40867|           78614|       18841|        CA|\n",
      "| Albany|   Georgia|      33.3|          31695|            39414|           71109|         861|        GA|\n",
      "| Albany|  New York|      32.8|          47627|            50825|           98452|       11948|        NY|\n",
      "+-------+----------+----------+---------------+-----------------+----------------+------------+----------+\n",
      "only showing top 6 rows\n",
      "\n",
      "root\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- median_age: double (nullable = true)\n",
      " |-- male_population: integer (nullable = true)\n",
      " |-- female_population: integer (nullable = true)\n",
      " |-- total_population: integer (nullable = true)\n",
      " |-- foreign_born: integer (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# I'm not interested in the following columns [number_of_veterans, average_household_size, race, count]\n",
    "cols_to_drop = ['number_of_veterans', 'average_household_size', 'race', 'count']\n",
    "usCitiesDemographicsDF = usCitiesDemographicsDF.drop(*cols_to_drop)\n",
    "# This will result in duplicated fields, because the data is partitioned by race, so I will remove duplicates by city and state\n",
    "usCitiesDemographicsDF = usCitiesDemographicsDF.dropDuplicates()\n",
    "usCitiesDemographicsDF.orderBy(usCitiesDemographicsDF.city, usCitiesDemographicsDF.state).show(6)\n",
    "usCitiesDemographicsDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "596"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usCitiesDemographicsDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write dataframe to parquet file\n",
    "usCitiesDemographicsDF.coalesce(1).write.parquet(\"us_cities_demographics/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+----------------+------------+----------+-----------------+\n",
      "|      city|               state|total_population|foreign_born|state_code|       state_name|\n",
      "+----------+--------------------+----------------+------------+----------+-----------------+\n",
      "|    Nashua|       New Hampshire|           87975|       12693|        NH|    NEW HAMPSHIRE|\n",
      "|  Carlsbad|          California|          113466|       17689|        CA|       CALIFORNIA|\n",
      "|  Missoula|             Montana|           71024|        2771|        MT|          MONTANA|\n",
      "|Fort Worth|               Texas|          836969|      143404|        TX|            TEXAS|\n",
      "|Washington|District of Columbia|          672228|       95117|        DC|DIST. OF COLUMBIA|\n",
      "+----------+--------------------+----------------+------------+----------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join usCitiesDemographics with i94 states lookup just to check if `state_code` matches\n",
    "usCitiesDemographicsWithStateNamesDF = usCitiesDemographicsDF.join(states_df, usCitiesDemographicsDF.state_code == states_df.state_code)\\\n",
    "                                        .select(\"city\",\\\n",
    "                                                \"state\",\\\n",
    "                                                \"total_population\",\\\n",
    "                                                \"foreign_born\",\\\n",
    "                                               usCitiesDemographicsDF.state_code,\\\n",
    "                                               states_df.state_name)\n",
    "usCitiesDemographicsWithStateNamesDF.show(5)\n",
    "# This means we can join US Cities with i94 data using state_code\n",
    "# Notice state = state_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Analysing Airport Codes Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: integer (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|ident|         type|                name|elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|         coordinates|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|  00A|     heliport|   Total Rf Heliport|          11|       NA|         US|     US-PA|    Bensalem|     00A|     null|       00A|-74.9336013793945...|\n",
      "| 00AA|small_airport|Aero B Ranch Airport|        3435|       NA|         US|     US-KS|       Leoti|    00AA|     null|      00AA|-101.473911, 38.7...|\n",
      "| 00AK|small_airport|        Lowell Field|         450|       NA|         US|     US-AK|Anchor Point|    00AK|     null|      00AK|-151.695999146, 5...|\n",
      "| 00AL|small_airport|        Epps Airpark|         820|       NA|         US|     US-AL|     Harvest|    00AL|     null|      00AL|-86.7703018188476...|\n",
      "| 00AR|       closed|Newport Hospital ...|         237|       NA|         US|     US-AR|     Newport|    null|     null|      null| -91.254898, 35.6087|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airportCodes_schema = StructType([\n",
    "    StructField('ident', StringType(), True),\n",
    "    StructField('type', StringType(), True),\n",
    "    StructField('name', StringType(), True),\n",
    "    StructField('elevation_ft', IntegerType(), True),\n",
    "    StructField('continent', StringType(), True),\n",
    "    StructField('iso_country', StringType(), True),\n",
    "    StructField('iso_region', StringType(), True),\n",
    "    StructField('municipality', StringType(), True),\n",
    "    StructField('gps_code', StringType(), True),\n",
    "    StructField('iata_code', StringType(), True),\n",
    "    StructField('local_code', StringType(), True),\n",
    "    StructField('coordinates', StringType(), True)\n",
    "])\n",
    "airportCodesDF = spark.read.csv(\"airport_codes.csv\",schema = airportCodes_schema,header = True)\n",
    "airportCodesDF.printSchema()\n",
    "airportCodesDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Create a new dataset out of Airport codes dataset with only the columns and rows in interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      "\n",
      "+-----+--------------------+-----------+----------+------------+--------+---------+----------+\n",
      "|ident|                name|iso_country|iso_region|municipality|gps_code|iata_code|local_code|\n",
      "+-----+--------------------+-----------+----------+------------+--------+---------+----------+\n",
      "|  00A|   Total Rf Heliport|         US|     US-PA|    Bensalem|     00A|     null|       00A|\n",
      "| 00AA|Aero B Ranch Airport|         US|     US-KS|       Leoti|    00AA|     null|      00AA|\n",
      "| 00AK|        Lowell Field|         US|     US-AK|Anchor Point|    00AK|     null|      00AK|\n",
      "| 00AL|        Epps Airpark|         US|     US-AL|     Harvest|    00AL|     null|      00AL|\n",
      "| 00AR|Newport Hospital ...|         US|     US-AR|     Newport|    null|     null|      null|\n",
      "+-----+--------------------+-----------+----------+------------+--------+---------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# I'm not interested in the following columns ['type', 'elevation_ft', 'continent', 'coordinates']\n",
    "cols_to_drop = ['type', 'elevation_ft', 'continent', 'coordinates']\n",
    "airportCodesDF = airportCodesDF.drop(*cols_to_drop)\n",
    "airportCodesDF.printSchema()\n",
    "airportCodesDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write dataframe to parquet file\n",
    "airportCodesDF.coalesce(1).write.parquet(\"airport_codes/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Performed cleaning tasks here\n",
    "More details can be found in section **3.2 Mapping Out Data Pipelines**, I will just abstract the cleaning here.\n",
    "- For **i94 immigration**:\n",
    "  - Some columns need to be dropped\n",
    "  - All column data type need to be set\n",
    "  - Some values for some columns need to be altered\n",
    "- For **World Tempreture**:\n",
    "  - Some columns need to be dropped\n",
    "  - Use average tempreture per city per country\n",
    "  - Join with `code_country` to get `countrycode` column\n",
    "- For **US Cities Demographics**:\n",
    "  - Some columns need to be dropped\n",
    "  - Remove duplicates\n",
    "- For **Airport Codes**:\n",
    "  - Some columns need to be dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "![Conceptual Data Model](images/conceptual_data_model.png)\n",
    "\n",
    "- The model consists of a fact table and 9 dimension tables\n",
    "- It represents a Star Schema, which will make the joining between tables easy and clear for a business user\n",
    "- Analytical queries are meant to be performed on the fact table\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model.\n",
    "\n",
    "##### i94_immigration Table\n",
    "- Use PySpark to read i94 immigration data from SAS files into a DataFrame, lets call it `df1`\n",
    "- Drop the following columns from `df1` as they are not needed \n",
    "  - count\n",
    "  - dtadfile\n",
    "  - visapost\n",
    "  - occup\n",
    "  - entdepa\n",
    "  - entdepd\n",
    "  - entdepu\n",
    "  - insnum\n",
    "  - validres\n",
    "  - delete_days\n",
    "  - delete_mexl\n",
    "  - delete_dup\n",
    "  - delete_visa\n",
    "  - delete_recdup\n",
    "- Cast all columns to their desired types, and adjust values of `i94mode` and `matflag`\n",
    "  - `i94mode`: Use 9 instead of null for missing values, use 9 instead of 0 for Not reported values\n",
    "  - `matflag`: Make it boolean true if value is M, false otherwise\n",
    "- Store the `df1` as Parquet file(s)\n",
    "- Upload Parquet files to AWS S3 using boto\n",
    "- Load the these files into the corresponding table in AWS RedShift\n",
    "\n",
    "##### Lookup Tables\n",
    "- Tables `code_country`, `ports`, `port_mode`, `states`, `visa_category` are static lookup tables.\n",
    "- Upload their files to AWS S3 using boto\n",
    "- Load each file to it's corresponding table in AWS RedShift\n",
    "\n",
    "##### i94_dates Table\n",
    "- We can auto-generate this table, however we don't want to create unnecessary records\n",
    "- Instead of auto-generating it, we can get all the date values from `i94_immigration` table and insert it into `i94_dates` table\n",
    "- We can do that after loading `i94_immigration`, we can run a query on AWS RedShift to do that for us\n",
    "\n",
    "##### world_temp Table\n",
    "- Load the CSV file into a DataFrame, lets call it `df1`\n",
    "- Drop the following columns from `df1` as they are not needed \n",
    "  - AverageTemperatureUncertainty\n",
    "  - Latitude\n",
    "  - Longitude\n",
    "- The dates in the dataset doesn't overlap with the `i94_immigration` dataset, for that we will use the **average tempreture per city, country**\n",
    "- Load `code_country` into a DataFrame and join it with `df1` on `country` to get the `Country_Code`\n",
    "  - _Notice:_ Code will be null for United states, since code_country.csv comes form the dataset of the immigration to the United states\n",
    "- Write `df1` to a parquet file\n",
    "- Upload the file to AWS S3 using boto\n",
    "- Load the file into it's corresponding table in AWS RedShift\n",
    "\n",
    "##### us_cities_demographics Table\n",
    "- Load the CSV file into a DataFrame, lets call it `df1`\n",
    "- Drop the following columns from `df1` as they are not needed \n",
    "  - number_of_veterans\n",
    "  - average_household_size\n",
    "  - race\n",
    "  - count\n",
    "- Dropping these columns will cause duplicated record, because there are repeated records and the only difference between them is `race` and `count`\n",
    "- Drop duplicates by `city` and `state`\n",
    "- Write `df1` to a parquet file\n",
    "- Upload the file to AWS S3 using boto\n",
    "- Load the records in this file to it's corresponding table in AWS Redshift\n",
    "\n",
    "##### airport_codes Table\n",
    "- Load the CSV file into a DataFrame, lets call it `df1`\n",
    "- Drop the following columns from `df1` as they are not needed \n",
    "  - type\n",
    "  - elevation_ft\n",
    "  - continent\n",
    "  - coordinates\n",
    "- Write `df1` to a parquet file\n",
    "- Upload the file to AWS S3 using boto\n",
    "- Load the records in this file to it's corresponding table in AWS Redshift\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import configparser\n",
    "import pandas as pd\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Read Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Param</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DWH_CLUSTER_TYPE</td>\n",
       "      <td>multi-node</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DWH_NUM_NODES</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DWH_NODE_TYPE</td>\n",
       "      <td>dc2.large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DWH_CLUSTER_IDENTIFIER</td>\n",
       "      <td>dwhCluster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DWH_DB</td>\n",
       "      <td>dwh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DWH_DB_USER</td>\n",
       "      <td>dwhuser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DWH_DB_PASSWORD</td>\n",
       "      <td>Passw0rd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>DWH_PORT</td>\n",
       "      <td>5439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DWH_IAM_ROLE_NAME</td>\n",
       "      <td>dwhRole</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>BUCKET_NAME</td>\n",
       "      <td>capstone-staging-area</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Param                  Value\n",
       "0        DWH_CLUSTER_TYPE             multi-node\n",
       "1           DWH_NUM_NODES                      4\n",
       "2           DWH_NODE_TYPE              dc2.large\n",
       "3  DWH_CLUSTER_IDENTIFIER             dwhCluster\n",
       "4                  DWH_DB                    dwh\n",
       "5             DWH_DB_USER                dwhuser\n",
       "6         DWH_DB_PASSWORD               Passw0rd\n",
       "7                DWH_PORT                   5439\n",
       "8       DWH_IAM_ROLE_NAME                dwhRole\n",
       "9             BUCKET_NAME  capstone-staging-area"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = configparser.ConfigParser()\n",
    "\n",
    "#Normally this file should be in ~/.aws/credentials\n",
    "config.read_file(open('aws/credentials.cfg'))\n",
    "\n",
    "KEY                    = config.get('AWS','KEY')\n",
    "SECRET                 = config.get('AWS','SECRET')\n",
    "\n",
    "DWH_CLUSTER_TYPE       = config.get(\"DWH\",\"DWH_CLUSTER_TYPE\")\n",
    "DWH_NUM_NODES          = config.get(\"DWH\",\"DWH_NUM_NODES\")\n",
    "DWH_NODE_TYPE          = config.get(\"DWH\",\"DWH_NODE_TYPE\")\n",
    "\n",
    "DWH_CLUSTER_IDENTIFIER = config.get(\"DWH\",\"DWH_CLUSTER_IDENTIFIER\")\n",
    "DWH_DB                 = config.get(\"DWH\",\"DWH_DB\")\n",
    "DWH_DB_USER            = config.get(\"DWH\",\"DWH_DB_USER\")\n",
    "DWH_DB_PASSWORD        = config.get(\"DWH\",\"DWH_DB_PASSWORD\")\n",
    "DWH_PORT               = config.get(\"DWH\",\"DWH_PORT\")\n",
    "\n",
    "DWH_IAM_ROLE_NAME      = config.get(\"DWH\", \"DWH_IAM_ROLE_NAME\")\n",
    "\n",
    "BUCKET_NAME            = config.get(\"S3\", \"BUCKET_NAME\")\n",
    "\n",
    "(DWH_DB_USER, DWH_DB_PASSWORD, DWH_DB)\n",
    "\n",
    "pd.DataFrame({\"Param\":\n",
    "                  [\"DWH_CLUSTER_TYPE\", \"DWH_NUM_NODES\", \"DWH_NODE_TYPE\", \"DWH_CLUSTER_IDENTIFIER\", \"DWH_DB\", \"DWH_DB_USER\", \"DWH_DB_PASSWORD\", \"DWH_PORT\", \"DWH_IAM_ROLE_NAME\",\"BUCKET_NAME\"],\n",
    "              \"Value\":\n",
    "                  [DWH_CLUSTER_TYPE, DWH_NUM_NODES, DWH_NODE_TYPE, DWH_CLUSTER_IDENTIFIER, DWH_DB, DWH_DB_USER, DWH_DB_PASSWORD, DWH_PORT, DWH_IAM_ROLE_NAME,BUCKET_NAME]\n",
    "             })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Upload files to AWS S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define S3 resource\n",
    "s3 = boto3.resource('s3',\n",
    "                       region_name=\"us-west-2\",\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Upload a single files to S3\n",
    "# Filename - File to upload\n",
    "# Bucket - Bucket to upload to (the top level directory under AWS S3)\n",
    "# Key - S3 object name (can contain subdirectories). If not specified then file_name is used\n",
    "# s3.meta.client.upload_file(Filename='airport_codes.csv', Bucket=BUCKET_NAME, Key='airport_codes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def uploadDirectory(path,bucketname):\n",
    "    print(\"Source file path = \"+path)\n",
    "    print(\"Destination bucket name = \"+bucketname)\n",
    "    for root,dirs,files in os.walk(path):\n",
    "        if root[0] == '.' or \"/.\" in root :\n",
    "            continue\n",
    "        print(\"======================\")\n",
    "        print(\"* Current root: \"+root)\n",
    "        print(\"---------------------\")\n",
    "        for file in files:\n",
    "            if file[0] == '_' or file[0] == '.':\n",
    "                continue\n",
    "            print(\"Current file: \"+file)\n",
    "            s3.meta.client.upload_file(os.path.join(root,file),bucketname,root+\"/\"+file)\n",
    "        print(\"======================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source file path = airport_codes\n",
      "Destination bucket name = capstone-staging-area\n",
      "--------------------\n",
      "Current root: airport_codes\n",
      "--------------------\n",
      "Current file: part-00000-4e6a3157-de28-452e-98c0-4ae050c9c6a2-c000.snappy.parquet\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# Upload airport_codes to S3 bucket\n",
    "uploadDirectory(\"airport_codes\",BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source file path = lookups\n",
      "Destination bucket name = capstone-staging-area\n",
      "======================\n",
      "* Current root: lookups\n",
      "---------------------\n",
      "Current file: code_country.csv\n",
      "Current file: states.csv\n",
      "Current file: port_mode.csv\n",
      "Current file: visa_category.csv\n",
      "Current file: ports.csv\n",
      "======================\n"
     ]
    }
   ],
   "source": [
    "# Upload lookups to S3 bucket\n",
    "uploadDirectory(\"lookups\",BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source file path = world_temp\n",
      "Destination bucket name = capstone-staging-area\n",
      "======================\n",
      "* Current root: world_temp\n",
      "---------------------\n",
      "Current file: part-00000-7762b432-aa71-420d-bfe7-1cb947f370d0-c000.snappy.parquet\n",
      "======================\n"
     ]
    }
   ],
   "source": [
    "# Upload world_temp to S3 bucket\n",
    "uploadDirectory(\"world_temp\",BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source file path = us_cities_demographics\n",
      "Destination bucket name = capstone-staging-area\n",
      "======================\n",
      "* Current root: us_cities_demographics\n",
      "---------------------\n",
      "Current file: part-00000-c259ffeb-d6a5-456f-af19-7e0541c363f3-c000.snappy.parquet\n",
      "======================\n"
     ]
    }
   ],
   "source": [
    "# Upload us_cities_demographics to S3 bucket\n",
    "uploadDirectory(\"us_cities_demographics\",BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source file path = i94_immigration\n",
      "Destination bucket name = capstone-staging-area\n",
      "======================\n",
      "* Current root: i94_immigration\n",
      "---------------------\n",
      "======================\n",
      "======================\n",
      "* Current root: i94_immigration/i94_sep16_sub\n",
      "---------------------\n",
      "Current file: part-00000-aae5cfd9-a9bf-4e74-8627-d5087c373435-c000.snappy.parquet\n",
      "======================\n",
      "======================\n",
      "* Current root: i94_immigration/i94_jan16_sub\n",
      "---------------------\n",
      "Current file: part-00000-7ab26318-2815-419c-9c79-f0c8a812a860-c000.snappy.parquet\n",
      "======================\n",
      "======================\n",
      "* Current root: i94_immigration/i94_apr16_sub\n",
      "---------------------\n",
      "Current file: part-00000-768f7644-5124-4875-9d4c-0dd71daa2a74-c000.snappy.parquet\n",
      "======================\n",
      "======================\n",
      "* Current root: i94_immigration/i94_dec16_sub\n",
      "---------------------\n",
      "Current file: part-00000-dc8ce8d3-d9ed-46ed-9449-3b3a50d6ac09-c000.snappy.parquet\n",
      "======================\n",
      "======================\n",
      "* Current root: i94_immigration/i94_nov16_sub\n",
      "---------------------\n",
      "Current file: part-00000-53a787ce-e633-4da7-bef2-487c7e64dabf-c000.snappy.parquet\n",
      "======================\n",
      "======================\n",
      "* Current root: i94_immigration/i94_may16_sub\n",
      "---------------------\n",
      "Current file: part-00000-9b1ffe6b-ada7-473d-82a6-46e7539a388a-c000.snappy.parquet\n",
      "======================\n",
      "======================\n",
      "* Current root: i94_immigration/i94_mar16_sub\n",
      "---------------------\n",
      "Current file: part-00000-b2642d3a-ec4d-4631-ad35-41aa8b218130-c000.snappy.parquet\n",
      "======================\n",
      "======================\n",
      "* Current root: i94_immigration/i94_jul16_sub\n",
      "---------------------\n",
      "Current file: part-00000-6c323bb2-74f0-46e6-adee-4c9662db35b6-c000.snappy.parquet\n",
      "======================\n",
      "======================\n",
      "* Current root: i94_immigration/i94_aug16_sub\n",
      "---------------------\n",
      "Current file: part-00000-63f1afa4-c3c2-4990-b731-cbf8d19f728f-c000.snappy.parquet\n",
      "======================\n",
      "======================\n",
      "* Current root: i94_immigration/i94_feb16_sub\n",
      "---------------------\n",
      "Current file: part-00000-b018a727-0d19-4736-ad1d-ea004cb25ae2-c000.snappy.parquet\n",
      "======================\n",
      "======================\n",
      "* Current root: i94_immigration/i94_jun16_sub\n",
      "---------------------\n",
      "Current file: part-00000-281b6091-9b8e-4d0d-8da8-7d355ed94d71-c000.snappy.parquet\n",
      "======================\n",
      "======================\n",
      "* Current root: i94_immigration/i94_oct16_sub\n",
      "---------------------\n",
      "Current file: part-00000-d330159e-52ab-4f00-a6bf-c3f9b5decf9c-c000.snappy.parquet\n",
      "======================\n"
     ]
    }
   ],
   "source": [
    "# Upload us_cities_demographics to S3 bucket\n",
    "uploadDirectory(\"i94_immigration\",BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Checking uploaded files to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3.ObjectSummary(bucket_name='capstone-staging-area', key='airport_codes/part-00000-4e6a3157-de28-452e-98c0-4ae050c9c6a2-c000.snappy.parquet')\n",
      "s3.ObjectSummary(bucket_name='capstone-staging-area', key='i94_immigration/i94_apr16_sub/part-00000-768f7644-5124-4875-9d4c-0dd71daa2a74-c000.snappy.parquet')\n",
      "s3.ObjectSummary(bucket_name='capstone-staging-area', key='i94_immigration/i94_aug16_sub/part-00000-63f1afa4-c3c2-4990-b731-cbf8d19f728f-c000.snappy.parquet')\n",
      "s3.ObjectSummary(bucket_name='capstone-staging-area', key='i94_immigration/i94_dec16_sub/part-00000-dc8ce8d3-d9ed-46ed-9449-3b3a50d6ac09-c000.snappy.parquet')\n",
      "s3.ObjectSummary(bucket_name='capstone-staging-area', key='i94_immigration/i94_feb16_sub/part-00000-b018a727-0d19-4736-ad1d-ea004cb25ae2-c000.snappy.parquet')\n",
      "s3.ObjectSummary(bucket_name='capstone-staging-area', key='i94_immigration/i94_jan16_sub/part-00000-7ab26318-2815-419c-9c79-f0c8a812a860-c000.snappy.parquet')\n",
      "s3.ObjectSummary(bucket_name='capstone-staging-area', key='i94_immigration/i94_jul16_sub/part-00000-6c323bb2-74f0-46e6-adee-4c9662db35b6-c000.snappy.parquet')\n",
      "s3.ObjectSummary(bucket_name='capstone-staging-area', key='i94_immigration/i94_jun16_sub/part-00000-281b6091-9b8e-4d0d-8da8-7d355ed94d71-c000.snappy.parquet')\n",
      "s3.ObjectSummary(bucket_name='capstone-staging-area', key='i94_immigration/i94_mar16_sub/part-00000-b2642d3a-ec4d-4631-ad35-41aa8b218130-c000.snappy.parquet')\n",
      "s3.ObjectSummary(bucket_name='capstone-staging-area', key='i94_immigration/i94_may16_sub/part-00000-9b1ffe6b-ada7-473d-82a6-46e7539a388a-c000.snappy.parquet')\n",
      "s3.ObjectSummary(bucket_name='capstone-staging-area', key='i94_immigration/i94_nov16_sub/part-00000-53a787ce-e633-4da7-bef2-487c7e64dabf-c000.snappy.parquet')\n",
      "s3.ObjectSummary(bucket_name='capstone-staging-area', key='i94_immigration/i94_oct16_sub/part-00000-d330159e-52ab-4f00-a6bf-c3f9b5decf9c-c000.snappy.parquet')\n",
      "s3.ObjectSummary(bucket_name='capstone-staging-area', key='i94_immigration/i94_sep16_sub/part-00000-aae5cfd9-a9bf-4e74-8627-d5087c373435-c000.snappy.parquet')\n",
      "s3.ObjectSummary(bucket_name='capstone-staging-area', key='lookups/code_country.csv')\n",
      "s3.ObjectSummary(bucket_name='capstone-staging-area', key='lookups/port_mode.csv')\n",
      "s3.ObjectSummary(bucket_name='capstone-staging-area', key='lookups/ports.csv')\n",
      "s3.ObjectSummary(bucket_name='capstone-staging-area', key='lookups/states.csv')\n",
      "s3.ObjectSummary(bucket_name='capstone-staging-area', key='lookups/visa_category.csv')\n",
      "s3.ObjectSummary(bucket_name='capstone-staging-area', key='us_cities_demographics/part-00000-c259ffeb-d6a5-456f-af19-7e0541c363f3-c000.snappy.parquet')\n",
      "s3.ObjectSummary(bucket_name='capstone-staging-area', key='world_temp/part-00000-7762b432-aa71-420d-bfe7-1cb947f370d0-c000.snappy.parquet')\n"
     ]
    }
   ],
   "source": [
    "# Check out filesin s3\n",
    "sampleDbBucket =  s3.Bucket(BUCKET_NAME)\n",
    "\n",
    "# Iterate over bucket objects starting with \"\" and print\n",
    "for obj in sampleDbBucket.objects.filter(Prefix=\"\"):\n",
    "    print(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Create AWS Redshift Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Define AWS resources\n",
    "ec2 = boto3.resource('ec2',\n",
    "                       region_name=\"us-west-2\",\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                    )\n",
    "iam = boto3.client('iam',aws_access_key_id=KEY,\n",
    "                     aws_secret_access_key=SECRET,\n",
    "                     region_name='us-west-2'\n",
    "                  )\n",
    "redshift = boto3.client('redshift',\n",
    "                       region_name=\"us-west-2\",\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Creating a new IAM Role\n",
      "An error occurred (EntityAlreadyExists) when calling the CreateRole operation: Role with name dwhRole already exists.\n"
     ]
    }
   ],
   "source": [
    "# Create an IAM Role that makes Redshift able to access S3 bucket (ReadOnly)\n",
    "try:\n",
    "    print('- Creating a new IAM Role')\n",
    "    dwhRole = iam.create_role(\n",
    "        Path='/',\n",
    "        RoleName=DWH_IAM_ROLE_NAME,\n",
    "        Description = \"Allows Redshift clusters to call AWS services on your behalf.\",\n",
    "        AssumeRolePolicyDocument=json.dumps(\n",
    "            {'Statement': [{'Action': 'sts:AssumeRole',\n",
    "               'Effect': 'Allow',\n",
    "               'Principal': {'Service': 'redshift.amazonaws.com'}}],\n",
    "             'Version': '2012-10-17'})\n",
    "    )    \n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Attaching Policy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Attach Policy\n",
    "print('- Attaching Policy')\n",
    "iam.attach_role_policy(RoleName=DWH_IAM_ROLE_NAME,\n",
    "                       PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\"\n",
    "                      )['ResponseMetadata']['HTTPStatusCode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Get the IAM role ARN\n",
      "arn:aws:iam::347115321620:role/dwhRole\n"
     ]
    }
   ],
   "source": [
    "# Get and print the IAM role ARN\n",
    "print('- Get the IAM role ARN')\n",
    "roleArn = iam.get_role(RoleName=DWH_IAM_ROLE_NAME)['Role']['Arn']\n",
    "print(roleArn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create a Redshift Cluster\n",
    "try:\n",
    "    response = redshift.create_cluster(        \n",
    "        # Add parameters for hardware\n",
    "        ClusterType=DWH_CLUSTER_TYPE,\n",
    "        NodeType=DWH_NODE_TYPE,\n",
    "        NumberOfNodes=int(DWH_NUM_NODES),\n",
    "        \n",
    "        # Add parameters for identifiers & credentials\n",
    "        DBName=DWH_DB,\n",
    "        ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,\n",
    "        MasterUsername=DWH_DB_USER,\n",
    "        MasterUserPassword=DWH_DB_PASSWORD,\n",
    "        \n",
    "        # Add parameter for role (to allow s3 access)\n",
    "        IamRoles=[roleArn] \n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> ClusterStatus = available\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Key</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ClusterIdentifier</td>\n",
       "      <td>dwhcluster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NodeType</td>\n",
       "      <td>dc2.large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ClusterStatus</td>\n",
       "      <td>available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MasterUsername</td>\n",
       "      <td>dwhuser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DBName</td>\n",
       "      <td>dwh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Endpoint</td>\n",
       "      <td>{'Address': 'dwhcluster.cjhgmfxe879j.us-west-2.redshift.amazonaws.com', 'Port': 5439}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>VpcId</td>\n",
       "      <td>vpc-23c1ba5b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NumberOfNodes</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Key  \\\n",
       "0  ClusterIdentifier   \n",
       "1  NodeType            \n",
       "2  ClusterStatus       \n",
       "3  MasterUsername      \n",
       "4  DBName              \n",
       "5  Endpoint            \n",
       "6  VpcId               \n",
       "7  NumberOfNodes       \n",
       "\n",
       "                                                                                   Value  \n",
       "0  dwhcluster                                                                             \n",
       "1  dc2.large                                                                              \n",
       "2  available                                                                              \n",
       "3  dwhuser                                                                                \n",
       "4  dwh                                                                                    \n",
       "5  {'Address': 'dwhcluster.cjhgmfxe879j.us-west-2.redshift.amazonaws.com', 'Port': 5439}  \n",
       "6  vpc-23c1ba5b                                                                           \n",
       "7  4                                                                                      "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "# Describe the cluster to see its status - run this block several times until the cluster status becomes Available\n",
    "def prettyRedshiftProps(props):\n",
    "    pd.set_option('display.max_colwidth', -1)\n",
    "    keysToShow = [\"ClusterIdentifier\", \"NodeType\", \"ClusterStatus\", \"MasterUsername\", \"DBName\", \"Endpoint\", \"NumberOfNodes\", 'VpcId']\n",
    "    x = [(k, v) for k,v in props.items() if k in keysToShow]\n",
    "    return pd.DataFrame(data=x, columns=[\"Key\", \"Value\"])\n",
    "\n",
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "# Busy wait until the cluster is created\n",
    "while myClusterProps[\"ClusterStatus\"] == \"creating\":\n",
    "    time.sleep(30) # Sleep 30 sec\n",
    "    myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "print(\"--> ClusterStatus = \"+myClusterProps[\"ClusterStatus\"])\n",
    "prettyRedshiftProps(myClusterProps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DWH_ENDPOINT ::  dwhcluster.cjhgmfxe879j.us-west-2.redshift.amazonaws.com\n",
      "DWH_ROLE_ARN ::  arn:aws:iam::347115321620:role/dwhRole\n"
     ]
    }
   ],
   "source": [
    "# Take note of the cluster endpoint and role ARN - DO NOT RUN THIS unless the cluster status becomes \"Available\"\n",
    "DWH_ENDPOINT = myClusterProps['Endpoint']['Address']\n",
    "DWH_ROLE_ARN = myClusterProps['IamRoles'][0]['IamRoleArn']\n",
    "print(\"DWH_ENDPOINT :: \", DWH_ENDPOINT)\n",
    "print(\"DWH_ROLE_ARN :: \", DWH_ROLE_ARN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# set new value\n",
    "config.set('RedShift', 'DWH_ENDPOINT', DWH_ENDPOINT)\n",
    "config.set('RedShift', 'DWH_ROLE_ARN', DWH_ROLE_ARN)\n",
    "\n",
    "# save the file\n",
    "with open('aws/credentials.cfg', 'w') as configfile:\n",
    "    config.write(configfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ec2.SecurityGroup(id='sg-010e12c359cf51059')\n",
      "An error occurred (InvalidPermission.Duplicate) when calling the AuthorizeSecurityGroupIngress operation: the specified rule \"peer: 0.0.0.0/0, TCP, from port: 5439, to port: 5439, ALLOW\" already exists\n"
     ]
    }
   ],
   "source": [
    "# Open an incoming TCP port to access the cluster ednpoint\n",
    "try:\n",
    "    vpc = ec2.Vpc(id=myClusterProps['VpcId'])\n",
    "    defaultSg = list(vpc.security_groups.all())[0]\n",
    "    print(defaultSg)\n",
    "    \n",
    "    defaultSg.authorize_ingress(\n",
    "        GroupName=defaultSg.group_name,\n",
    "        CidrIp='0.0.0.0/0',\n",
    "        IpProtocol='TCP',\n",
    "        FromPort=int(DWH_PORT),\n",
    "        ToPort=int(DWH_PORT)\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Connected: dwhuser@dwh'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure you can connect to the cluster\n",
    "%load_ext sql\n",
    "from time import time\n",
    "conn_string=\"postgresql://{}:{}@{}:{}/{}\".format(DWH_DB_USER, DWH_DB_PASSWORD, DWH_ENDPOINT, DWH_PORT,DWH_DB)\n",
    "#print(conn_string)\n",
    "%sql $conn_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Create tables in Redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.cjhgmfxe879j.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql \n",
    "DROP TABLE IF EXISTS \"airport_codes\";\n",
    "CREATE TABLE \"airport_codes\" (\n",
    "    \"ident\" VARCHAR NOT NULL,\n",
    "    \"name\" VARCHAR,\n",
    "    \"iso_country\" VARCHAR,\n",
    "    \"iso_region\" VARCHAR,\n",
    "    \"municipality\" VARCHAR,\n",
    "    \"gps_code\" VARCHAR,\n",
    "    \"iata_code\" VARCHAR,\n",
    "    \"local_code\" VARCHAR\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.cjhgmfxe879j.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql \n",
    "DROP TABLE IF EXISTS \"code_country\";\n",
    "CREATE TABLE \"code_country\" (\n",
    "    \"code\" INTEGER NOT NULL,\n",
    "    \"country\" VARCHAR NOT NULL\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.cjhgmfxe879j.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql \n",
    "DROP TABLE IF EXISTS \"ports\";\n",
    "CREATE TABLE \"ports\" (\n",
    "    \"code\" VARCHAR NOT NULL,\n",
    "    \"port\" VARCHAR NOT NULL,\n",
    "    \"state_code\" VARCHAR NOT NULL\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.cjhgmfxe879j.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql \n",
    "DROP TABLE IF EXISTS \"port_mode\";\n",
    "CREATE TABLE \"port_mode\" (\n",
    "    \"code\" INTEGER NOT NULL,\n",
    "    \"mode\" VARCHAR NOT NULL\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.cjhgmfxe879j.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql \n",
    "DROP TABLE IF EXISTS \"states\";\n",
    "CREATE TABLE \"states\" (\n",
    "    \"state_code\" VARCHAR NOT NULL,\n",
    "    \"state_name\" VARCHAR NOT NULL\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.cjhgmfxe879j.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql \n",
    "DROP TABLE IF EXISTS \"i94_dates\";\n",
    "CREATE TABLE \"i94_dates\" (\n",
    "    \"full_date\" DATE NOT NULL,\n",
    "    \"day\" INTEGER NOT NULL,\n",
    "    \"month\" INTEGER NOT NULL,\n",
    "    \"year\" INTEGER NOT NULL,\n",
    "    \"week\" INTEGER NOT NULL,\n",
    "    \"weekday\" VARCHAR NOT NULL\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.cjhgmfxe879j.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql \n",
    "DROP TABLE IF EXISTS \"visa_category\";\n",
    "CREATE TABLE \"visa_category\" (\n",
    "    \"code\" INTEGER NOT NULL,\n",
    "    \"visa_category\" VARCHAR NOT NULL\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.cjhgmfxe879j.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql \n",
    "DROP TABLE IF EXISTS \"world_temp\";\n",
    "CREATE TABLE \"world_temp\" (\n",
    "    \"City\" VARCHAR NOT NULL,\n",
    "    \"Country\" VARCHAR NOT NULL,\n",
    "    \"AvgTemp\" DOUBLE PRECISION,\n",
    "    \"Country_Code\" INTEGER\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.cjhgmfxe879j.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql \n",
    "DROP TABLE IF EXISTS \"us_cities_demographics\";\n",
    "CREATE TABLE \"us_cities_demographics\" (\n",
    "    \"city\" VARCHAR NOT NULL,\n",
    "    \"state\" VARCHAR NOT NULL,\n",
    "    \"median_age\" DOUBLE PRECISION,\n",
    "    \"male_population\" INTEGER,\n",
    "    \"female_population\" INTEGER,\n",
    "    \"total_population\" INTEGER,\n",
    "    \"foreign_born\" INTEGER,\n",
    "    \"state_code\" VARCHAR\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.cjhgmfxe879j.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql \n",
    "DROP TABLE IF EXISTS \"i94_immigration\";\n",
    "CREATE TABLE \"i94_immigration\" (\n",
    "    \"cicid\" BIGINT NOT NULL,\n",
    "    \"i94yr\" INTEGER NOT NULL,\n",
    "    \"i94mon\" INTEGER NOT NULL,\n",
    "    \"i94cit\" INTEGER,\n",
    "    \"i94res\" INTEGER,\n",
    "    \"i94port\" VARCHAR,\n",
    "    \"arrdate\" DATE,\n",
    "    \"i94mode\" INTEGER,\n",
    "    \"i94addr\" VARCHAR,\n",
    "    \"depdate\" DATE,\n",
    "    \"i94bir\" INTEGER,\n",
    "    \"i94visa\" INTEGER,\n",
    "    \"biryear\" INTEGER,\n",
    "    \"dtaddto\" VARCHAR,\n",
    "    \"gender\" VARCHAR,\n",
    "    \"airline\" VARCHAR,\n",
    "    \"admnum\" BIGINT,\n",
    "    \"fltno\" VARCHAR,\n",
    "    \"visatype\" VARCHAR,\n",
    "    \"matflag\" BOOLEAN\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Load data into tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.cjhgmfxe879j.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n",
      "CPU times: user 4.47 ms, sys: 70 µs, total: 4.54 ms\n",
      "Wall time: 2.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "qry = \"\"\"\n",
    "    COPY airport_codes\n",
    "    FROM 's3://capstone-staging-area/airport_codes/'\n",
    "    credentials 'aws_iam_role={}'\n",
    "    FORMAT AS PARQUET;\n",
    "\"\"\".format(DWH_ROLE_ARN)\n",
    "\n",
    "%sql $qry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.cjhgmfxe879j.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n",
      "CPU times: user 4.9 ms, sys: 140 µs, total: 5.04 ms\n",
      "Wall time: 1.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "qry = \"\"\"\n",
    "    COPY code_country\n",
    "    FROM 's3://capstone-staging-area/lookups/code_country.csv'\n",
    "    credentials 'aws_iam_role={}'\n",
    "    delimiter ',' CSV QUOTE '\"' IGNOREHEADER 1 compupdate off region 'us-west-2';\n",
    "\"\"\".format(DWH_ROLE_ARN)\n",
    "\n",
    "%sql $qry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.cjhgmfxe879j.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n",
      "CPU times: user 1.2 ms, sys: 3.54 ms, total: 4.73 ms\n",
      "Wall time: 1.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "qry = \"\"\"\n",
    "    COPY ports\n",
    "    FROM 's3://capstone-staging-area/lookups/ports.csv'\n",
    "    credentials 'aws_iam_role={}'\n",
    "    delimiter ',' CSV QUOTE '\"' IGNOREHEADER 1 compupdate off region 'us-west-2';\n",
    "\"\"\".format(DWH_ROLE_ARN)\n",
    "\n",
    "%sql $qry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.cjhgmfxe879j.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n",
      "CPU times: user 1.77 ms, sys: 3.58 ms, total: 5.35 ms\n",
      "Wall time: 812 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "qry = \"\"\"\n",
    "    COPY port_mode\n",
    "    FROM 's3://capstone-staging-area/lookups/port_mode.csv'\n",
    "    credentials 'aws_iam_role={}'\n",
    "    delimiter ',' CSV QUOTE '\"' IGNOREHEADER 1 compupdate off region 'us-west-2';\n",
    "\"\"\".format(DWH_ROLE_ARN)\n",
    "\n",
    "%sql $qry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.cjhgmfxe879j.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n",
      "CPU times: user 532 µs, sys: 4.14 ms, total: 4.67 ms\n",
      "Wall time: 1.29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "qry = \"\"\"\n",
    "    COPY states\n",
    "    FROM 's3://capstone-staging-area/lookups/states.csv'\n",
    "    credentials 'aws_iam_role={}'\n",
    "    delimiter ',' CSV QUOTE '\"' IGNOREHEADER 1 compupdate off region 'us-west-2';\n",
    "\"\"\".format(DWH_ROLE_ARN)\n",
    "\n",
    "%sql $qry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.cjhgmfxe879j.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n",
      "CPU times: user 3.87 ms, sys: 523 µs, total: 4.39 ms\n",
      "Wall time: 799 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "qry = \"\"\"\n",
    "    COPY visa_category\n",
    "    FROM 's3://capstone-staging-area/lookups/visa_category.csv'\n",
    "    credentials 'aws_iam_role={}'\n",
    "    delimiter ',' CSV QUOTE '\"' IGNOREHEADER 1 compupdate off region 'us-west-2';\n",
    "\"\"\".format(DWH_ROLE_ARN)\n",
    "\n",
    "%sql $qry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.cjhgmfxe879j.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n",
      "CPU times: user 6.5 ms, sys: 0 ns, total: 6.5 ms\n",
      "Wall time: 14.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "qry = \"\"\"\n",
    "    COPY world_temp\n",
    "    FROM 's3://capstone-staging-area/world_temp/'\n",
    "    credentials 'aws_iam_role={}'\n",
    "    FORMAT AS PARQUET;\n",
    "\"\"\".format(DWH_ROLE_ARN)\n",
    "\n",
    "%sql $qry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.cjhgmfxe879j.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n",
      "CPU times: user 4.52 ms, sys: 0 ns, total: 4.52 ms\n",
      "Wall time: 1.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "qry = \"\"\"\n",
    "    COPY us_cities_demographics\n",
    "    FROM 's3://capstone-staging-area/us_cities_demographics/'\n",
    "    credentials 'aws_iam_role={}'\n",
    "    FORMAT AS PARQUET;\n",
    "\"\"\".format(DWH_ROLE_ARN)\n",
    "\n",
    "%sql $qry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.cjhgmfxe879j.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n",
      "CPU times: user 6.84 ms, sys: 3.86 ms, total: 10.7 ms\n",
      "Wall time: 35.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "qry = \"\"\"\n",
    "    COPY i94_immigration\n",
    "    FROM 's3://capstone-staging-area/i94_immigration/i94'\n",
    "    credentials 'aws_iam_role={}'\n",
    "    FORMAT AS PARQUET;\n",
    "\"\"\".format(DWH_ROLE_ARN)\n",
    "\n",
    "%sql $qry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Load i94_dates dates from i94_immigration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.cjhgmfxe879j.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "732 rows affected.\n",
      "CPU times: user 8.67 ms, sys: 98 µs, total: 8.77 ms\n",
      "Wall time: 1.36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "qry = \"\"\"\n",
    "INSERT INTO i94_dates\n",
    "SELECT  dt, \n",
    "        EXTRACT(DAY FROM dt) as day, \n",
    "        EXTRACT(MONTH FROM dt) as month, \n",
    "        EXTRACT(YEAR FROM dt) as year, \n",
    "        EXTRACT(WEEK FROM  dt) as week, \n",
    "        to_char(dt,'Day') as weekday\n",
    "FROM\n",
    "(\n",
    "    SELECT DISTINCT dt\n",
    "    FROM\n",
    "    (\n",
    "            SELECT DISTINCT arrdate as dt\n",
    "            FROM i94_immigration\n",
    "            WHERE arrdate IS NOT NULL\n",
    "        UNION\n",
    "            SELECT DISTINCT depdate as dt\n",
    "            FROM i94_immigration\n",
    "            WHERE depdate IS NOT NULL\n",
    "    ) \n",
    ") as t4\n",
    "\"\"\"\n",
    "\n",
    "%sql $qry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Delete created AWS Redshift Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Cluster': {'ClusterIdentifier': 'dwhcluster',\n",
       "  'NodeType': 'dc2.large',\n",
       "  'ClusterStatus': 'deleting',\n",
       "  'MasterUsername': 'dwhuser',\n",
       "  'DBName': 'dwh',\n",
       "  'Endpoint': {'Address': 'dwhcluster.cjhgmfxe879j.us-west-2.redshift.amazonaws.com',\n",
       "   'Port': 5439},\n",
       "  'ClusterCreateTime': datetime.datetime(2020, 12, 26, 22, 21, 6, 736000, tzinfo=tzlocal()),\n",
       "  'AutomatedSnapshotRetentionPeriod': 1,\n",
       "  'ClusterSecurityGroups': [],\n",
       "  'VpcSecurityGroups': [{'VpcSecurityGroupId': 'sg-b9bf0890',\n",
       "    'Status': 'active'}],\n",
       "  'ClusterParameterGroups': [{'ParameterGroupName': 'default.redshift-1.0',\n",
       "    'ParameterApplyStatus': 'in-sync'}],\n",
       "  'ClusterSubnetGroupName': 'default',\n",
       "  'VpcId': 'vpc-23c1ba5b',\n",
       "  'AvailabilityZone': 'us-west-2b',\n",
       "  'PreferredMaintenanceWindow': 'sun:10:30-sun:11:00',\n",
       "  'PendingModifiedValues': {},\n",
       "  'ClusterVersion': '1.0',\n",
       "  'AllowVersionUpgrade': True,\n",
       "  'NumberOfNodes': 4,\n",
       "  'PubliclyAccessible': True,\n",
       "  'Encrypted': False,\n",
       "  'Tags': [],\n",
       "  'EnhancedVpcRouting': False,\n",
       "  'IamRoles': [{'IamRoleArn': 'arn:aws:iam::347115321620:role/dwhRole',\n",
       "    'ApplyStatus': 'in-sync'}],\n",
       "  'MaintenanceTrackName': 'current'},\n",
       " 'ResponseMetadata': {'RequestId': '763cb3ab-6ff0-4f5c-bb97-4ce63e919a6c',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '763cb3ab-6ff0-4f5c-bb97-4ce63e919a6c',\n",
       "   'content-type': 'text/xml',\n",
       "   'content-length': '2541',\n",
       "   'vary': 'accept-encoding',\n",
       "   'date': 'Sat, 26 Dec 2020 23:13:28 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### CAREFUL!!\n",
    "#-- Uncomment & run to delete the created resources\n",
    "redshift.delete_cluster( ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,  SkipFinalClusterSnapshot=True)\n",
    "#### CAREFUL!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClusterStatus = deleting\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Key</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ClusterIdentifier</td>\n",
       "      <td>dwhcluster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NodeType</td>\n",
       "      <td>dc2.large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ClusterStatus</td>\n",
       "      <td>deleting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MasterUsername</td>\n",
       "      <td>dwhuser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DBName</td>\n",
       "      <td>dwh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Endpoint</td>\n",
       "      <td>{'Address': 'dwhcluster.cjhgmfxe879j.us-west-2.redshift.amazonaws.com', 'Port': 5439}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>VpcId</td>\n",
       "      <td>vpc-23c1ba5b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NumberOfNodes</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Key  \\\n",
       "0  ClusterIdentifier   \n",
       "1  NodeType            \n",
       "2  ClusterStatus       \n",
       "3  MasterUsername      \n",
       "4  DBName              \n",
       "5  Endpoint            \n",
       "6  VpcId               \n",
       "7  NumberOfNodes       \n",
       "\n",
       "                                                                                   Value  \n",
       "0  dwhcluster                                                                             \n",
       "1  dc2.large                                                                              \n",
       "2  deleting                                                                               \n",
       "3  dwhuser                                                                                \n",
       "4  dwh                                                                                    \n",
       "5  {'Address': 'dwhcluster.cjhgmfxe879j.us-west-2.redshift.amazonaws.com', 'Port': 5439}  \n",
       "6  vpc-23c1ba5b                                                                           \n",
       "7  4                                                                                      "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run this block several times until the cluster really deleted\n",
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "k = prettyRedshiftProps(myClusterProps)\n",
    "print(\"ClusterStatus = \"+myClusterProps[\"ClusterStatus\"])\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#### CAREFUL!!\n",
    "#-- Uncomment & run to delete the iam role\n",
    "#iam.detach_role_policy(RoleName=DWH_IAM_ROLE_NAME, PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\")\n",
    "#iam.delete_role(RoleName=DWH_IAM_ROLE_NAME)\n",
    "#### CAREFUL!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Perform quality checks here\n",
    "#import psycopg2\n",
    "#try:\n",
    "#    con=psycopg2.connect(dbname= DWH_DB, host=DWH_ENDPOINT, port= DWH_PORT, user= DWH_DB_USER, password= DWH_DB_PASSWORD)\n",
    "#except psycopg2.Error as e: \n",
    "#    print(\"Error: Could not make connection to the Redshift database\")\n",
    "#    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#try:\n",
    "#    cur = con.cursor()\n",
    "#except psycopg2.Error as e: \n",
    "#    print(\"Error: Could not get curser to the Database\")\n",
    "#    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#con.set_session(autocommit = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#cur.execute(\"SELECT * FROM airport_codes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "#data = np.array(cur.fetchall())\n",
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#cur.close() \n",
    "#conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>name</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00AA</td>\n",
       "      <td>Aero B Ranch Airport</td>\n",
       "      <td>US</td>\n",
       "      <td>US-KS</td>\n",
       "      <td>Leoti</td>\n",
       "      <td>00AA</td>\n",
       "      <td>None</td>\n",
       "      <td>00AA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00CN</td>\n",
       "      <td>Kitchen Creek Helibase Heliport</td>\n",
       "      <td>US</td>\n",
       "      <td>US-CA</td>\n",
       "      <td>Pine Valley</td>\n",
       "      <td>00CN</td>\n",
       "      <td>None</td>\n",
       "      <td>00CN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ident                             name iso_country iso_region municipality  \\\n",
       "0  00AA  Aero B Ranch Airport             US          US-KS      Leoti         \n",
       "1  00CN  Kitchen Creek Helibase Heliport  US          US-CA      Pine Valley   \n",
       "\n",
       "  gps_code iata_code local_code  \n",
       "0  00AA     None      00AA       \n",
       "1  00CN     None      00CN       "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "# Test connection to Redshift using Pandas\n",
    "engine = create_engine('postgresql://'+DWH_DB_USER+':'+DWH_DB_PASSWORD+'@'+DWH_ENDPOINT+':'+DWH_PORT+'/'+DWH_DB)\n",
    "data_frame = pd.read_sql('SELECT * FROM airport_codes LIMIT 2', engine)\n",
    "data_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Source/Count checks to ensure completeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_count(table_name):\n",
    "    counter_pdf = pd.read_sql('SELECT count(*) as counter FROM '+table_name, engine)\n",
    "    return counter_pdf.iloc[0]['counter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# airport_codes count = 55,075\n",
    "# code_country count = 289\n",
    "# ports count = 661\n",
    "# port_mode count = 4\n",
    "# states count = 55\n",
    "# visa_category count = 3\n",
    "# world_temp count = 3490\n",
    "# us_cities_demographics count = 596\n",
    "# i94_immigration count = 40790529\n",
    "# i94_dates count = 732\n",
    "assert (get_count('airport_codes') == 55075), \"Error: airport_codes is not 55075\"\n",
    "assert (get_count('code_country') == 289), \"Error: code_country is not 289\"\n",
    "assert (get_count('ports') == 661), \"Error: ports is not 661\"\n",
    "assert (get_count('port_mode') == 4), \"Error: port_mode is not 4\"\n",
    "assert (get_count('states') == 55), \"Error: states is not 55\"\n",
    "assert (get_count('visa_category') == 3), \"Error: visa_category is not 3\"\n",
    "assert (get_count('world_temp') == 3490), \"Error: world_temp is not 3490\"\n",
    "assert (get_count('us_cities_demographics') == 596), \"Error: us_cities_demographics is not 596\"\n",
    "assert (get_count('i94_immigration') == 40790529), \"Error: i94_immigration is not 40790529\"\n",
    "assert (get_count('i94_dates') == 732), \"Error: i94_dates is not 732\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Integrity constraints on the relational database (e.g., unique key, data type, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_duplicate_pks_count(table_name, pks):\n",
    "    pks_pdf = pd.read_sql(\"\"\"\n",
    "                        SELECT count(*) as counter \n",
    "                        FROM (\n",
    "                            SELECT \"\"\"+pks+\"\"\", count(*)\n",
    "                            FROM \"\"\"+table_name+\"\"\"\n",
    "                            GROUP BY \"\"\"+pks+\"\"\"\n",
    "                            HAVING count(*) > 1\n",
    "                        )\n",
    "                        \"\"\", engine)\n",
    "    return pks_pdf.iloc[0]['counter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "assert (get_duplicate_pks_count('airport_codes','ident') == 0), \"Error: duplicate PKs in airport_codes\"\n",
    "assert (get_duplicate_pks_count('code_country','code') == 0), \"Error: duplicate PKs in code_country\"\n",
    "assert (get_duplicate_pks_count('ports','code') == 0), \"Error: duplicate PKs in ports\"\n",
    "assert (get_duplicate_pks_count('port_mode','code') == 0), \"Error: duplicate PKs in port_mode\"\n",
    "assert (get_duplicate_pks_count('states','state_code') == 0), \"Error: duplicate PKs in states\"\n",
    "assert (get_duplicate_pks_count('visa_category','code') == 0), \"Error: duplicate PKs in visa_category\"\n",
    "assert (get_duplicate_pks_count('world_temp','city,country') == 0), \"Error: duplicate PKs in world_temp\"\n",
    "assert (get_duplicate_pks_count('us_cities_demographics','city,state') == 0), \"Error: duplicate PKs in us_cities_demographics\"\n",
    "assert (get_duplicate_pks_count('i94_immigration','cicid,i94yr,i94mon') == 0), \"Error: duplicate PKs in i94_immigration\"\n",
    "assert (get_duplicate_pks_count('i94_dates','full_date') == 0), \"Error: duplicate PKs in i94_dates\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file.\n",
    "\n",
    "##### i94_immigration\n",
    "| Column   | Type    | Description                                                                                                                                                                                                                  |\n",
    "|----------|---------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| cicid    | long    | Id, part of the composite primary key.                                                                                                                                                                                       |\n",
    "| i94yr    | integer | 4 digit year of the arrival, part of the composite primary key.                                                                                                                                                              |\n",
    "| i94mon   | integer | Numeric month of the arrival, part of the composite primary key.                                                                                                                                                             |\n",
    "| i94cit   | integer | Visitor Country of Citizenship (3 digit code of origin country)                                                                                                                                                              |\n",
    "| i94res   | integer | Visitor country of residence                                                                                                                                                                                                 |\n",
    "| i94port  | string  | Port of entry, can be in the USA or outside USA. (City, State if USA/Country otherwise) 3 character code of destination city --> Foreign key City Temperature (City) --> Foreign key U.S. City Demographics (City and State) |\n",
    "| arrdate  | date    | Arrival Date in the USA, It is a SAS date numeric field                                                                                                                                                                      |\n",
    "| i94mode  | integer | Port mode (Air, Sea, Land, Not reported + missing values). 1 digit travel code                                                                                                                                               |\n",
    "| i94addr  | string  | Landing state. --> Foreign Key to the U.S. City Demographics (State)                                                                                                                                                         |\n",
    "| depdate  | date    | Departure Date from the USA. It is a SAS date numeric field                                                                                                                                                                  |\n",
    "| i94bir   | integer | Age of Respondent in Years                                                                                                                                                                                                   |\n",
    "| i94visa  | integer | Visa code (Business, Pleasure, Student). Reason for immigration                                                                                                                                                              |\n",
    "| biryear  | integer | 4 digit year of birth                                                                                                                                                                                                        |\n",
    "| dtaddto  | string  | Character Date Field - Date to which admitted to U.S. (allowed to stay until)                                                                                                                                                |\n",
    "| gender   | string  | Non-immigrant sex                                                                                                                                                                                                            |\n",
    "| airline  | string  | Code of the Airline used to arrive in U.S.  --> Foreign Key to Airport Codes, IATA, and Local Code --> Should also use municipality column in Airport Codes                                                                  |\n",
    "| admnum   | long    | Admission Number is the number on a CBP Form I–94 or CBP Form I–94W, Does this mean it's unique? The answer is no                                                                                                            |\n",
    "| fltno    | string  | Flight number of Airline used to arrive in U.S.                                                                                                                                                                              |\n",
    "| visatype | string  | (B1, B2 .. etc) Class of admission legally admitting the non-immigrant to temporarily stay in U.S.                                                                                                                           |\n",
    "| matflag  | boolean | Match flag - Match of arrival and departure records (If value missing -> No depature date)                                                                                                                                   |\n",
    "\n",
    "##### i94_dates\n",
    "| Column    | Type    | Description                                     |\n",
    "|-----------|---------|-------------------------------------------------|\n",
    "| full_date | date    | Primary key. Full date in the format YYYY-MM-DD |\n",
    "| day       | integer | Day of the month of full_date                   |\n",
    "| month     | integer | Month of the full_date                          |\n",
    "| year      | integer | Year of the full_date                           |\n",
    "| week      | integer | Week of the full_date                           |\n",
    "| weekday   | string  | Weekday of the full_date                        |\n",
    "\n",
    "##### code_country\n",
    "| Column  | Type    | Description                                                               |\n",
    "|---------|---------|---------------------------------------------------------------------------|\n",
    "| code    | integer | Primary key. 3 digit code for I94CIT & I94RES in i94_immigration dataset. |\n",
    "| country | string  | Country name                                                              |\n",
    "\n",
    "##### ports\n",
    "| Column     | Type   | Description                                                         |\n",
    "|------------|--------|---------------------------------------------------------------------|\n",
    "| code       | string | Primary key. 3 letters Port code for I94PORT in immigration dataset |\n",
    "| port       | string | Port/City name                                                      |\n",
    "| state_code | string | State code if the port in US otherwise Country name                 |\n",
    "\n",
    "##### port_mode\n",
    "| Column | Type    | Description                                      |\n",
    "|--------|---------|--------------------------------------------------|\n",
    "| code   | integer | Primary Key. For I94MODE in immigration dataset. |\n",
    "| mode   | string  | Can be 'Air', 'Sea', 'Land', and 'Not Reported'  |\n",
    "\n",
    "##### states\n",
    "| Column     | Type   | Description                                                                         |\n",
    "|------------|--------|-------------------------------------------------------------------------------------|\n",
    "| state_code | string | Primary key. For I94ADDR in immigration dataset. Repesent the code of the US state. |\n",
    "| state_name | string | Name of the US state                                                                |\n",
    "\n",
    "##### visa_category\n",
    "| Column        | Type   | Description                                           |\n",
    "|---------------|--------|-------------------------------------------------------|\n",
    "| code          | string | Primary key. Code for I94VISA in immigration dataset. |\n",
    "| visa_category | string | Can be 'Business', 'Pleasure', and 'Student'          |\n",
    "\n",
    "##### world_temp\n",
    "| Column      | Type    | Description                                      |\n",
    "|-------------|---------|--------------------------------------------------|\n",
    "| city        | string  | City name. Part of the composite primary key.    |\n",
    "| country     | string  | Country name. Part of the composite primary key. |\n",
    "| avgtemp     | double  | Average tempreture in the city                   |\n",
    "| countrycode | integer | Country code as in code_country table            |\n",
    "\n",
    "##### us_cities_demographics\n",
    "| Column            | Type    | Description                                                   |\n",
    "|-------------------|---------|---------------------------------------------------------------|\n",
    "| city              | string  | City within USA. Part of the composite primary key.           |\n",
    "| state             | string  | The USA State of the city. Part of the composite primary key. |\n",
    "| median_age        | double  | Median of the Ages of the people living in that city          |\n",
    "| male_population   | integer | Number of males in the city (Can be null)                     |\n",
    "| female_population | integer | Number of females in the city (Can be null)                   |\n",
    "| total_population  | integer | Sum of male and female population                             |\n",
    "| foreign_born      | integer | USA Veterans count                                            |\n",
    "| state_code        | string  | Code of the sate                                              |\n",
    "\n",
    "##### airport_codes\n",
    "| Column       | Type   | Description                                                    |\n",
    "|--------------|--------|----------------------------------------------------------------|\n",
    "| ident        | string | Primary key                                                    |\n",
    "| name         | string | Airport name                                                   |\n",
    "| iso_country  | string | Code for country of the Airport - US                           |\n",
    "| iso_region   | string | Code for the region of the Airport - Contains state code US-CA |\n",
    "| municipality | string | City of the Airport - City within the state                    |\n",
    "| gps_code     | string | Airport GPS code (Contains nulls)                              |\n",
    "| iata_code    | string | IATA airport code (Contains nulls)                             |\n",
    "| local_code   | string | ICAO airport code (Contains nulls)                             |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Choice of tools and technologies for the project\n",
    "- PySpark\n",
    "  - Spark makes representing the data as a DataFrame and wrangling it an easy task.\n",
    "  - Spark can write DataFrame to parquet files.\n",
    "- Parquet files:\n",
    "  - I choose parquet format because it provide good file compaction, so the file size will be reduced dramatically.\n",
    "  - Parquet is a columnar representation of the data, which will allow analytical queries to run fast.\n",
    "  - RedShift can read from parquet.\n",
    "- AWS S3\n",
    "  - S3 is considered a staging area to allow RedShift to read the files uploaded there.\n",
    "- AWS RedShift\n",
    "  - Amazon's Datawarehouse service enables high performance for queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### How often the data should be updated and why?\n",
    "\n",
    "- `i94_immigration`, this can be updated monthly since new data is provided on a monthly base. We just need to load the delta (the new records) only.\n",
    "- The number of records in the Lookup tables is small, this means we can overwrite the whole table in case of new values.\n",
    "- `airport_codes` can be updated daily. It contains few number of records so it can be fully overwritten to ensure consistency.\n",
    "- `world_temp` for this table I'm calculating the average tempreture per city per country, the number of resulted records out of this aggregation makes the size of the table small. This means we can overwrite the whole table in case of new updates.\n",
    "- `us_cities_demographics` this table is also small, and we can overwrite it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### What if the data was increased by 100x?\n",
    "\n",
    "In that case, we can't use PySpark on the Udacity Workspace. Instead, we can use AWS EMR, upload the data to AWS S3, then process it using Spark on our EMR cluster. This will also allow us to write directly to RedShift instead of using S3 as a staging area.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### What if the data populates a dashboard that must be updated on a daily basis by 7am every day?\n",
    "\n",
    "Since the most frequently updated data is the `airport_codes` (daily), then this should not be a problem. The dashboard can read directly from RedShift, meanwhile, our pipelines do any required update at 00:00 AM. This will ensure that the dashboard always contains the most recently available data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### What if The database needed to be accessed by 100+ people?\n",
    "\n",
    "AWS Redshift supports 100+ connection with no issue, only one restriction a maximum of 50 concurrent queries at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
